{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypotheses Testing in Econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of OLS Estimator\n",
    "Ordinary Least Squares (OLS) estimators must meet specific properties for valid and unbiased estimation. These properties ensure the regression model performs well in empirical applications.\n",
    "\n",
    "### 1. **Linearity**\n",
    "   - The OLS estimator is a linear combination of the observed dependent variable ($ Y $).\n",
    "   - The regression model takes the form:\n",
    "     $$\n",
    "     Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "     $$\n",
    "\n",
    "### 2. **Full Rank**\n",
    "   - The independent variables ($ X $) must be linearly independent.\n",
    "   - No multicollinearity among predictors.\n",
    "\n",
    "### 3. **Regression Model**\n",
    "   - The relationship between $ Y $ and $ X $ is correctly specified, and no relevant variables are omitted.\n",
    "\n",
    "### 4. **Spherical Errors**\n",
    "   - Errors ($ \\epsilon $) must have constant variance (homoscedasticity) and no autocorrelation.\n",
    "   - Mathematically:\n",
    "     $$\n",
    "     Var(\\epsilon) = \\sigma^2 I\n",
    "     $$\n",
    "\n",
    "### 5. **Non-Stochastic Regressors**\n",
    "   - The regressors ($ X $) are fixed or independent of the error term ($ \\epsilon $).\n",
    "\n",
    "### 6. **Normal Errors**\n",
    "   - Errors ($ \\epsilon $) are normally distributed:\n",
    "     $$\n",
    "     \\epsilon \\sim N(0, \\sigma^2)\n",
    "     $$\n",
    "   - Required for valid hypothesis testing using $ t $- and $ F $-statistics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Properties of OLS Estimators\n",
    "\n",
    "Ordinary Least Squares (OLS) estimators have three key properties: **Unbiasedness**, **Efficiency**, and **Consistency**. These properties ensure that OLS estimators are reliable for estimating relationships in data.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Unbiasedness**\n",
    "### Definition:\n",
    "An estimator is **unbiased** if its expected value equals the true value of the parameter:\n",
    "$$\n",
    "E(\\hat{\\beta}) = \\beta\n",
    "$$\n",
    "\n",
    "### Key Points:\n",
    "- OLS estimators are unbiased when the following assumptions are met:\n",
    "  1. Linearity of the model ($ Y = X\\beta + \\epsilon $).\n",
    "  2. Zero mean of the error term ($ E(\\epsilon) = 0 $).\n",
    "  3. No correlation between regressors ($ X $) and the error term ($ Cov(X, \\epsilon) = 0 $).\n",
    "  4. No omitted variables.\n",
    "  5. Non-stochastic regressors (independent variables are fixed or independent of errors).\n",
    "\n",
    "### Importance:\n",
    "- Ensures that the OLS estimates, on average, reflect the true relationship between variables.\n",
    "- Eliminates systematic errors in estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Efficiency**\n",
    "### Definition:\n",
    "An estimator is **efficient** if it has the smallest variance among all linear unbiased estimators, making it the most precise:\n",
    "$$\n",
    "Var(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "### Key Points:\n",
    "- OLS is efficient under the **Gauss-Markov assumptions**:\n",
    "  1. Linearity of the model.\n",
    "  2. Homoscedasticity (constant variance of errors).\n",
    "  3. No autocorrelation (errors are uncorrelated).\n",
    "  4. No omitted variables.\n",
    "  5. Non-stochastic regressors.\n",
    "\n",
    "### Importance:\n",
    "- Efficiency ensures precise estimates, improving the reliability of predictions and hypothesis tests.\n",
    "- When violated (e.g., in the presence of heteroscedasticity or autocorrelation), OLS is no longer efficient, but alternative methods like Generalized Least Squares (GLS) can be used.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Consistency**\n",
    "### Definition:\n",
    "An estimator is **consistent** if it converges to the true value of the parameter as the sample size increases:\n",
    "$$\n",
    "\\hat{\\beta} \\xrightarrow{p} \\beta \\quad \\text{as } n \\to \\infty\n",
    "$$\n",
    "\n",
    "### Key Points:\n",
    "- OLS is consistent when:\n",
    "  1. The model is correctly specified.\n",
    "  2. No omitted variables.\n",
    "  3. The regressors ($ X $) are not perfectly multicollinear.\n",
    "  4. The error term ($ \\epsilon $) is independent of the regressors ($ Cov(X, \\epsilon) = 0 $).\n",
    "  5. The sample size is sufficiently large.\n",
    "\n",
    "### Importance:\n",
    "- Consistency ensures that OLS estimates become increasingly accurate as more data is collected, making it a powerful tool for large-sample analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of the Properties\n",
    "\n",
    "| **Property**   | **Definition**                                                                 | **Key Assumptions**                                              | **Importance**                                                                                 |\n",
    "|-----------------|---------------------------------------------------------------------------------|------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|\n",
    "| **Unbiasedness** | The estimatorâ€™s expected value equals the true parameter.                     | Linearity, zero mean errors, no omitted variables, independent errors.                        | Ensures accurate estimation on average.                                                       |\n",
    "| **Efficiency**   | The estimator has the smallest variance among linear unbiased estimators.     | Linearity, homoscedasticity, no autocorrelation, no omitted variables.                        | Provides the most precise estimates.                                                          |\n",
    "| **Consistency**  | The estimator converges to the true value as sample size increases.           | Correct model specification, no omitted variables, large sample size, independent errors.     | Guarantees accurate estimates in large samples.                                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauss-Markov Theorem\n",
    "\n",
    "The **Gauss-Markov Theorem** is a fundamental result in statistics and econometrics. It states that under certain assumptions, the **Ordinary Least Squares (OLS)** estimator is the **Best Linear Unbiased Estimator (BLUE)** for the coefficients in a linear regression model.\n",
    "\n",
    "\n",
    "\n",
    "## Key Terms in the Gauss-Markov Theorem\n",
    "- **Best**: The estimator has the smallest variance among all linear unbiased estimators.\n",
    "- **Linear**: The estimator is a linear function of the observed data.\n",
    "- **Unbiased**: The expected value of the estimator equals the true value of the parameter:\n",
    "  $$\n",
    "  E(\\hat{\\beta}) = \\beta\n",
    "  $$\n",
    "- **Estimator**: A rule or formula to calculate estimates for unknown parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions for the Gauss-Markov Theorem\n",
    "For the OLS estimator to be BLUE, the following assumptions must hold:\n",
    "\n",
    "1. **Linearity of the Model**:\n",
    "   - The regression model is:\n",
    "     $$\n",
    "     Y = X\\beta + \\epsilon\n",
    "     $$\n",
    "     Where:\n",
    "     - $ Y $: Dependent variable (vector of size $ n \\times 1 $).\n",
    "     - $ X $: Independent variables (matrix of size $ n \\times k $).\n",
    "     - $ \\beta $: Coefficient vector ($ k \\times 1 $).\n",
    "     - $ \\epsilon $: Error term ($ n \\times 1 $).\n",
    "\n",
    "2. **Full Rank of $ X $**:\n",
    "   - The independent variables are linearly independent, ensuring the matrix $ X'X $ is invertible.\n",
    "\n",
    "3. **Zero Mean of Errors**:\n",
    "   - The error term has an expected value of zero:\n",
    "     $$\n",
    "     E(\\epsilon) = 0\n",
    "     $$\n",
    "\n",
    "4. **Homoscedasticity (Equal Variance of Errors)**:\n",
    "   - The variance of the errors is constant:\n",
    "     $$\n",
    "     Var(\\epsilon) = \\sigma^2 I\n",
    "     $$\n",
    "     Where $ I $ is the identity matrix.\n",
    "\n",
    "5. **No Autocorrelation**:\n",
    "   - The errors are uncorrelated:\n",
    "     $$\n",
    "     Cov(\\epsilon_i, \\epsilon_j) = 0 \\quad \\text{for } i \\neq j\n",
    "     $$\n",
    "\n",
    "6. **Non-Stochastic Regressors**:\n",
    "   - The independent variables $ X $ are fixed in repeated samples (or independent of the errors).\n",
    "\n",
    "---\n",
    "\n",
    "## Statement of the Gauss-Markov Theorem\n",
    "Under these assumptions:\n",
    "- The OLS estimator:\n",
    "  $$\n",
    "  \\hat{\\beta} = (X'X)^{-1} X'Y\n",
    "  $$\n",
    "  is **BLUE**:\n",
    "  - **Best**: It has the smallest variance among all linear unbiased estimators.\n",
    "  - **Linear**: It is a linear function of $ Y $.\n",
    "  - **Unbiased**: Its expected value equals the true value of the parameter ($ E(\\hat{\\beta}) = \\beta $).\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind the Gauss-Markov Theorem\n",
    "1. **Unbiasedness**:\n",
    "   - The OLS estimator accurately reflects the true relationship between the variables on average.\n",
    "   \n",
    "2. **Efficiency**:\n",
    "   - Among all linear and unbiased estimators, OLS has the smallest variance, meaning it produces the most precise estimates.\n",
    "\n",
    "3. **Practical Impact**:\n",
    "   - The OLS estimator is reliable and robust when the assumptions hold, making it a cornerstone of regression analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## What Happens If Assumptions Are Violated?\n",
    "1. **Homoscedasticity Violated**:\n",
    "   - If the error variance is not constant (heteroscedasticity), OLS is still unbiased but no longer efficient.\n",
    "   - Alternative methods like **Generalized Least Squares (GLS)** are preferred.\n",
    "\n",
    "2. **Autocorrelation**:\n",
    "   - If errors are correlated, OLS is unbiased but not efficient. Time-series models or corrections like **Newey-West standard errors** are used.\n",
    "\n",
    "3. **Model Misspecification**:\n",
    "   - If the model is misspecified or important variables are omitted, the OLS estimator becomes biased.\n",
    "\n",
    "---\n",
    "\n",
    "The Gauss-Markov Theorem is the foundation of linear regression analysis, justifying the use of OLS estimators under the specified assumptions. It ensures that OLS estimators are the best choice for estimating linear relationships, provided the assumptions hold. When the assumptions are violated, other methods or adjustments must be employed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypthesis Testing\n",
    "## t-test Approach\n",
    "\n",
    "The $ t $-test is a statistical method used to determine if there is a significant difference between the population parameter and a hypothesized value, or between two populations, using sample data. It is commonly used in regression analysis to test the significance of individual coefficients.\n",
    "\n",
    "---\n",
    "\n",
    "## **Purpose of the $ t $-Test**\n",
    "- To determine whether a single regression coefficient ($ \\beta $) is significantly different from zero or another value.\n",
    "- Helps assess whether an independent variable has a significant impact on the dependent variable in a regression model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Steps in Hypothesis Testing with $ t $-Test**\n",
    "\n",
    "### 1. **State the Hypotheses**\n",
    "- **Null Hypothesis ($ H_0 $)**: The parameter has no effect (e.g., $ \\beta = 0 $).\n",
    "- **Alternative Hypothesis ($ H_a $)**: The parameter has a significant effect (e.g., $ \\beta \\neq 0 $).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Calculate the Test Statistic**\n",
    "- The formula for the $ t $-statistic is:\n",
    "  $$\n",
    "  t = \\frac{\\hat{\\beta} - \\beta_0}{\\text{SE}(\\hat{\\beta})}\n",
    "  $$\n",
    "  Where:\n",
    "  - $ \\hat{\\beta} $: Estimated coefficient.\n",
    "  - $ \\beta_0 $: Hypothesized value (often 0).\n",
    "  - $ \\text{SE}(\\hat{\\beta}) $: Standard error of the coefficient.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Determine the Critical Value or $ p $-Value**\n",
    "- **Critical Value Approach**:\n",
    "  - Compare the calculated $ t $-statistic to the critical $ t $-value from the $ t $-distribution table at a given significance level ($ \\alpha $).\n",
    "- **$ p $-Value Approach**:\n",
    "  - Compute the $ p $-value corresponding to the $ t $-statistic.\n",
    "  - If $ p $-value $ < \\alpha $, reject $ H_0 $.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Decision Rule**\n",
    "- **If $ |t| > \\text{Critical Value} $ or $ p $-value $ < \\alpha $**:\n",
    "  - Reject $ H_0 $.\n",
    "  - Conclude that the parameter is statistically significant.\n",
    "- **If $ |t| \\leq \\text{Critical Value} $ or $ p $-value $ \\geq \\alpha $**:\n",
    "  - Fail to reject $ H_0 $.\n",
    "  - Conclude that there is no evidence to suggest the parameter is significant.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example of $ t $-Test in Regression**\n",
    "- Suppose we estimate a regression model:\n",
    "  $$\n",
    "  Y = \\beta_0 + \\beta_1 X + \\epsilon\n",
    "  $$\n",
    "- The estimated coefficient for $ X $ ($ \\hat{\\beta}_1 $) is tested with:\n",
    "  - $ H_0: \\beta_1 = 0 $ (no effect of $ X $ on $ Y $).\n",
    "  - $ H_a: \\beta_1 \\neq 0 $ (effect of $ X $ on $ Y $).\n",
    "\n",
    "---\n",
    "\n",
    "## **Interpretation of Results**\n",
    "- **$ t $-Statistic**:\n",
    "  - A high $ t $-statistic (positive or negative) suggests strong evidence against $ H_0 $.\n",
    "  - A low $ t $-statistic suggests insufficient evidence to reject $ H_0 $.\n",
    "- **$ p $-Value**:\n",
    "  - A small $ p $-value ($ < \\alpha $) indicates that the coefficient is statistically significant.\n",
    "- **Significance**:\n",
    "  - If the null hypothesis is rejected, the variable significantly affects the dependent variable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Assumptions of the $ t $-Test**\n",
    "1. The residuals ($ \\epsilon $) are normally distributed.\n",
    "2. The regression model is correctly specified.\n",
    "3. Homoscedasticity (constant variance of residuals).\n",
    "4. Independence of observations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| **Component**       | **Explanation**                                                                                  |\n",
    "|----------------------|--------------------------------------------------------------------------------------------------|\n",
    "| Null Hypothesis ($ H_0 $) | The parameter or coefficient is not statistically significant (e.g., $ \\beta = 0 $).              |\n",
    "| Alternative Hypothesis ($ H_a $) | The parameter or coefficient is statistically significant (e.g., $ \\beta \\neq 0 $).            |\n",
    "| Test Statistic ($ t $) | Measures the difference between the observed and hypothesized coefficient relative to its standard error. |\n",
    "| Significance Level ($ \\alpha $) | The threshold for rejecting $ H_0 $ (e.g., 0.05 or 5%).                                       |\n",
    "| $ p $-Value        | The probability of observing a result as extreme as the $ t $-statistic, assuming $ H_0 $ is true.        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  wlnyw   R-squared:                       0.199\n",
      "Model:                            OLS   Adj. R-squared:                  0.189\n",
      "Method:                 Least Squares   F-statistic:                     19.16\n",
      "Date:                Sat, 14 Dec 2024   Prob (F-statistic):           3.71e-08\n",
      "Time:                        12:33:07   Log-Likelihood:                -257.48\n",
      "No. Observations:                 157   AIC:                             521.0\n",
      "Df Residuals:                     154   BIC:                             530.1\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.2743      1.115      8.320      0.000       7.072      11.476\n",
      "lnwi           0.8482      0.222      3.829      0.000       0.411       1.286\n",
      "lnndg         -1.5861      0.407     -3.899      0.000      -2.390      -0.783\n",
      "==============================================================================\n",
      "Omnibus:                       10.520   Durbin-Watson:                   1.899\n",
      "Prob(Omnibus):                  0.005   Jarque-Bera (JB):                5.525\n",
      "Skew:                           0.256   Prob(JB):                       0.0631\n",
      "Kurtosis:                       2.237   Cond. No.                         42.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "T-statistic for a=0: 8.320432752141933\n",
      "T-statistic for b=0: 3.8292857065618104\n",
      "T-statistic for c=0: -3.8993531743413454\n",
      "Wald Test for b = -c:\n",
      "<F test: F=array([[2.14133291]]), p=0.14541532629385023, df_denom=154, df_num=1>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moeth\\miniconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:1912: FutureWarning: The behavior of wald_test will change after 0.14 to returning scalar test statistic values. To get the future behavior now, set scalar to True. To silence this message while retaining the legacy behavior, set scalar to False.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = \"data/solow_2000.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna()  # Drop rows with missing values\n",
    "\n",
    "# Define the independent and dependent variables\n",
    "X = df[['lnwi', 'lnndg']]\n",
    "X = sm.add_constant(X)  # Add intercept term\n",
    "y = df['wlnyw']\n",
    "\n",
    "# Estimate the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Display model summary (t-test for coefficients a=0, b=0, c=0)\n",
    "print(model.summary())\n",
    "\n",
    "##### Project 1: Hypothesis Testing #####\n",
    "\n",
    "# Coefficients and Standard Errors\n",
    "coefficients = model.params\n",
    "std_errors = model.bse\n",
    "\n",
    "# Test a=0 with t-test\n",
    "t_a = coefficients['const'] / std_errors['const']\n",
    "print(f\"T-statistic for a=0: {t_a}\")\n",
    "\n",
    "# Test b=0 with t-test\n",
    "t_b = coefficients['lnwi'] / std_errors['lnwi']\n",
    "print(f\"T-statistic for b=0: {t_b}\")\n",
    "\n",
    "# Test c=0 with t-test\n",
    "t_c = coefficients['lnndg'] / std_errors['lnndg']\n",
    "print(f\"T-statistic for c=0: {t_c}\")\n",
    "\n",
    "# Test b = -c with Wald test\n",
    "# Create a linear constraint: lnwi + lnndg = 0\n",
    "constraint_matrix = np.array([[0, 1, 1]])  # Coefficients for const, lnwi, lnndg\n",
    "wald_test_result = model.wald_test(constraint_matrix)\n",
    "print(f\"Wald Test for b = -c:\\n{wald_test_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solow Growth Model and Hypothesis Testing\n",
    "\n",
    "The Solow Growth Model is represented as:\n",
    "\n",
    "$$\n",
    "\\ln y_i = \\alpha + \\beta \\ln s_i + \\gamma \\ln (n + \\delta + g)_i + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y$: Income per capita\n",
    "- $s$: Saving rate\n",
    "- $n, \\delta, g$: Population growth rate, depreciation rate, and technology growth, respectively\n",
    "- $\\alpha, \\beta, \\gamma$: Model coefficients to be estimated\n",
    "- $\\epsilon$: Error term\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis Testing\n",
    "\n",
    "### Hypotheses\n",
    "We aim to test the following hypotheses using $t$-tests:\n",
    "1. $H_0: \\alpha = 0$\n",
    "2. $H_0: \\beta = 0$\n",
    "3. $H_0: \\gamma = 0$\n",
    "4. $H_0: \\beta = -\\gamma$\n",
    "\n",
    "---\n",
    "\n",
    "### Estimation and Tests\n",
    "1. **Estimate the Model**:\n",
    "   Using Ordinary Least Squares (OLS):\n",
    "   $$\n",
    "   \\ln y_i = \\alpha + \\beta \\ln s_i + \\gamma \\ln (n + \\delta + g)_i + \\epsilon_i\n",
    "   $$\n",
    "\n",
    "2. **Perform $t$-Tests for Individual Coefficients**:\n",
    "   - $t$-statistic is calculated as:\n",
    "     $$\n",
    "     t = \\frac{\\text{Coefficient}}{\\text{Standard Error}}\n",
    "     $$\n",
    "   - Reject $H_0$ if $p$-value $< 0.05$.\n",
    "\n",
    "3. **Test $H_0: \\beta = -\\gamma$ Using Wald Test**:\n",
    "   - Define the constraint $\\beta + \\gamma = 0$.\n",
    "   - Perform a Wald test to evaluate this hypothesis.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of OLS Regression Results\n",
    "\n",
    "## Model Summary\n",
    "- **Dependent Variable**: `wlnyw` (log of income per worker)\n",
    "- **Independent Variables**:\n",
    "  - `lnwi`: Log of investment rate\n",
    "  - `lnndg`: Log of population growth + depreciation rate + technology growth\n",
    "- **Model Fit**:\n",
    "  - $R^2 = 0.199$: Approximately 19.9% of the variability in the dependent variable (`wlnyw`) is explained by the independent variables.\n",
    "  - Adjusted $R^2 = 0.189$: Adjusts for the number of predictors, indicating a slightly lower explanatory power.\n",
    "  - $F-statistic = 19.16$: Tests the overall significance of the model.\n",
    "  - $p-value = 3.71 \\times 10^{-8}$: The model is statistically significant as the $p$-value is much smaller than 0.05.\n",
    "\n",
    "---\n",
    "\n",
    "## Coefficients and Their Interpretations\n",
    "\n",
    "| Variable | Coefficient | Std. Error | $t$-statistic | $p$-value | 95% Confidence Interval |\n",
    "|----------|-------------|------------|---------------|-----------|-------------------------|\n",
    "| const    | 9.2743      | 1.115      | 8.320         | 0.000     | [7.072, 11.476]        |\n",
    "| lnwi     | 0.8482      | 0.222      | 3.829         | 0.000     | [0.411, 1.286]         |\n",
    "| lnndg    | -1.5861     | 0.407      | -3.899        | 0.000     | [-2.390, -0.783]       |\n",
    "\n",
    "### Interpretation of Coefficients\n",
    "1. **Intercept (`const`)**:\n",
    "   - Coefficient = 9.2743.\n",
    "   - Represents the expected value of `wlnyw` (log of income per worker) when all independent variables (`lnwi` and `lnndg`) are zero.\n",
    "   - Highly significant with $t$-statistic = 8.32 and $p < 0.0001$.\n",
    "\n",
    "2. **Log of Investment Rate (`lnwi`)**:\n",
    "   - Coefficient = 0.8482.\n",
    "   - A 1% increase in the investment rate is associated with a 0.848% increase in income per worker.\n",
    "   - Statistically significant ($p < 0.0001$).\n",
    "\n",
    "3. **Log of Population Growth + Depreciation Rate + Technology Growth (`lnndg`)**:\n",
    "   - Coefficient = -1.5861.\n",
    "   - A 1% increase in population growth, depreciation, and technology growth collectively is associated with a 1.586% decrease in income per worker.\n",
    "   - Statistically significant ($p < 0.0001$).\n",
    "\n",
    "---\n",
    "\n",
    "## Diagnostics and Additional Tests\n",
    "\n",
    "1. **Omnibus Test**:\n",
    "   - $p = 0.005$: Indicates some deviation from normality in the residuals.\n",
    "   - However, the residuals are not highly skewed, with a Jarque-Bera $p$-value of 0.0631 (close to 0.05).\n",
    "\n",
    "2. **Durbin-Watson Statistic**:\n",
    "   - Value = 1.899.\n",
    "   - Indicates no strong autocorrelation in the residuals.\n",
    "\n",
    "3. **Condition Number**:\n",
    "   - Value = 42.6.\n",
    "   - Suggests that multicollinearity is not a significant concern in this model.\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis Tests for Coefficients\n",
    "\n",
    "### Individual Hypotheses\n",
    "\n",
    "1. **$H_0: \\alpha = 0$ (Intercept)**:\n",
    "   - $t = 8.32$, $p < 0.0001$.\n",
    "   - Reject $H_0$: There is strong evidence that the intercept is not zero.\n",
    "\n",
    "2. **$H_0: \\beta = 0$ (`lnwi`)**:\n",
    "   - $t = 3.829$, $p < 0.0001$.\n",
    "   - Reject $H_0$: The investment rate significantly affects income per worker.\n",
    "\n",
    "3. **$H_0: \\gamma = 0$ (`lnndg`)**:\n",
    "   - $t = -3.899$, $p < 0.0001$.\n",
    "   - Reject $H_0$: Population growth, depreciation, and technology growth significantly affect income per worker.\n",
    "\n",
    "---\n",
    "\n",
    "### Wald Test for $\\beta = -\\gamma$\n",
    "- Null Hypothesis: $\\beta + \\gamma = 0$.\n",
    "- Result:\n",
    "  - $F = 2.141$, $p = 0.145$.\n",
    "  - Fail to reject $H_0$: There is no strong evidence that $\\beta = -\\gamma$.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "- Both `lnwi` (investment rate) and `lnndg` (population growth + depreciation + technology growth) are statistically significant predictors of income per worker.\n",
    "- The positive coefficient for `lnwi` suggests that higher investment rates lead to higher income per worker, while the negative coefficient for `lnndg` implies that faster growth in population and depreciation reduces income per worker.\n",
    "- The overall model explains about 20% of the variation in income per worker, with significant coefficients and no major issues of multicollinearity or autocorrelation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Test in Regression Analysis\n",
    "\n",
    "## What is the F-Test?\n",
    "The **F-Test** is a statistical test used to evaluate the overall significance of a regression model or to test specific hypotheses about the relationships between variables. It helps us determine whether the independent variables in a model collectively explain a significant portion of the variability in the dependent variable.\n",
    "\n",
    "---\n",
    "\n",
    "## When is the F-Test Used?\n",
    "1. **Testing Overall Model Significance**:\n",
    "   - Determines if the regression model provides a better fit than a model with no predictors.\n",
    "   - Null Hypothesis ($H_0$): All coefficients except the intercept are zero.\n",
    "   - Alternative Hypothesis ($H_a$): At least one coefficient is not zero.\n",
    "\n",
    "2. **Comparing Two Nested Models**:\n",
    "   - Tests if adding additional predictors significantly improves the model fit.\n",
    "   - Null Hypothesis ($H_0$): The simpler model is as good as the more complex one.\n",
    "   - Alternative Hypothesis ($H_a$): The more complex model significantly improves the fit.\n",
    "\n",
    "---\n",
    "\n",
    "## Formula for the F-Test\n",
    "The F-statistic is calculated as:\n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{Explained Variance (Model)}}{\\text{Unexplained Variance (Residuals)}} \\cdot \\frac{\\text{(n - k - 1)}}{\\text{k}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$: Number of observations\n",
    "- $k$: Number of predictors\n",
    "- Explained Variance: Variation in the dependent variable explained by the model\n",
    "- Unexplained Variance: Variation left unexplained by the model (residuals)\n",
    "\n",
    "---\n",
    "\n",
    "## Hypotheses\n",
    "1. **Null Hypothesis ($H_0$)**:\n",
    "   - All regression coefficients (except the intercept) are equal to zero.\n",
    "   - The independent variables do not explain any variation in the dependent variable.\n",
    "\n",
    "2. **Alternative Hypothesis ($H_a$)**:\n",
    "   - At least one regression coefficient is not zero.\n",
    "   - The independent variables collectively explain a significant amount of variation in the dependent variable.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Rule\n",
    "- Compare the computed F-statistic to the critical value from the F-distribution table (based on degrees of freedom).\n",
    "- Alternatively, use the $p$-value:\n",
    "  - If $p \\leq \\alpha$ (e.g., 0.05), **reject $H_0$** (model is significant).\n",
    "  - If $p > \\alpha$, **fail to reject $H_0$** (model is not significant).\n",
    "\n",
    "---\n",
    "\n",
    "## Interpretation of Results\n",
    "1. **Significant F-Test**:\n",
    "   - Indicates that the regression model provides a better fit than a model with no predictors.\n",
    "   - Suggests that at least one independent variable contributes to explaining the dependent variable.\n",
    "\n",
    "2. **Non-Significant F-Test**:\n",
    "   - Implies that the independent variables collectively do not explain a significant portion of the dependent variable's variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "If you have a regression model:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "The F-Test will check whether:\n",
    "\n",
    "$$\n",
    "H_0: \\beta_1 = \\beta_2 = 0\n",
    "$$\n",
    "\n",
    "- A significant F-statistic means $X_1$ and/or $X_2$ explain significant variability in $Y$.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of the F-Test\n",
    "1. **Model Comparison**:\n",
    "   - Used to determine if a more complex model is better than a simpler model.\n",
    "   \n",
    "2. **Overall Significance**:\n",
    "   - Helps validate whether the model as a whole is useful.\n",
    "\n",
    "3. **ANOVA (Analysis of Variance)**:\n",
    "   - Used to compare the means across multiple groups.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "- The F-Test is a **global test** to assess the overall model, while $t$-tests are used to evaluate individual coefficients.\n",
    "- The F-Test assumes that the residuals are normally distributed and the model is correctly specified.\n",
    "- A large F-statistic or a small $p$-value indicates a significant relationship between the predictors and the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   df_resid         ssr  df_diff    ss_diff          F    Pr(>F)\n",
      "0     155.0  268.426829      0.0        NaN        NaN       NaN\n",
      "1     154.0  244.305680      1.0  24.121149  15.204955  0.000144\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# Load the data\n",
    "file_path = \"data/solow_2000.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define the independent and dependent variables for the Solow model\n",
    "# wlnyw = ln per capita income\n",
    "# lnwi = ln savings rate\n",
    "# lnndg = ln (population growth rate + depreciation + technology growth)\n",
    "X = df[['lnwi', 'lnndg']]  # Independent variables\n",
    "y = df['wlnyw']            # Dependent variable\n",
    "\n",
    "# Add a constant for the intercept in the regression model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the full model\n",
    "model_full = sm.OLS(y, X).fit()\n",
    "\n",
    "# Fit the restricted model (assume coefficients of independent variables are zero)\n",
    "# Restricted model: Only includes the intercept (no predictors)\n",
    "X_restricted = sm.add_constant(df[['lnwi']])  # Include only one predictor\n",
    "model_restricted = sm.OLS(y, X_restricted).fit()\n",
    "\n",
    "# Perform the F-Test\n",
    "anova_results = anova_lm(model_restricted, model_full)\n",
    "\n",
    "# Print the ANOVA results (includes the F-statistic and p-value)\n",
    "print(anova_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting F-Test Results for the Solow Growth Model\n",
    "\n",
    "### Output Table\n",
    "| Model  | df_resid | ssr          | df_diff | ss_diff     | F         | Pr(>F)   |\n",
    "|--------|----------|--------------|---------|-------------|-----------|----------|\n",
    "| 0      | 155.0    | 268.426829   | 0.0     | NaN         | NaN       | NaN      |\n",
    "| 1      | 154.0    | 244.305680   | 1.0     | 24.121149   | 15.204955 | 0.000144 |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Values Explained\n",
    "\n",
    "1. **`df_resid` (Residual Degrees of Freedom)**:\n",
    "   - Degrees of freedom left after fitting the model.\n",
    "   - Model 0 (restricted): 155 residual degrees of freedom.\n",
    "   - Model 1 (full): 154 residual degrees of freedom (1 less because an additional variable was added).\n",
    "\n",
    "2. **`ssr` (Sum of Squared Residuals)**:\n",
    "   - Measures how much of the dependent variable's variability remains unexplained.\n",
    "   - Model 0 (restricted): `ssr = 268.43` (higher, worse fit).\n",
    "   - Model 1 (full): `ssr = 244.31` (lower, better fit).\n",
    "\n",
    "3. **`df_diff` (Difference in Degrees of Freedom)**:\n",
    "   - Shows the number of additional parameters in the full model compared to the restricted model.\n",
    "   - Here, `df_diff = 1`, meaning one extra variable was added in the full model.\n",
    "\n",
    "4. **`ss_diff` (Difference in SSR)**:\n",
    "   - The reduction in the sum of squared residuals from the restricted to the full model.\n",
    "   - Here, `ss_diff = 24.12`, indicating that adding the new variable reduced unexplained variability by 24.12 units.\n",
    "\n",
    "5. **`F` (F-statistic)**:\n",
    "   - Tests whether the improvement in the model is statistically significant.\n",
    "   - Here, `F = 15.20`, a large value, indicating a significant improvement in the model fit.\n",
    "\n",
    "6. **`Pr(>F)` (p-value)**:\n",
    "   - The probability of observing such a large F-statistic under the null hypothesis.\n",
    "   - Here, `p = 0.000144`, which is much smaller than 0.05, so we reject the null hypothesis.\n",
    "\n",
    "---\n",
    "\n",
    "### Hypotheses\n",
    "- **Null Hypothesis ($H_0$)**:\n",
    "  - The additional variable does not improve the model (its coefficient is zero).\n",
    "\n",
    "- **Alternative Hypothesis ($H_a$)**:\n",
    "  - The additional variable improves the model fit (its coefficient is not zero).\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "- Since the **p-value ($0.000144$)** is less than **0.05**, we reject the null hypothesis.\n",
    "- The additional variable in the full model significantly improves the explanation of income per worker ($wlnyw$).\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Implication\n",
    "- Adding the variable (e.g., $\\ln(n + \\delta + g)$, representing population growth, depreciation, and technology growth) to the model significantly enhances its predictive power.\n",
    "- This suggests that both savings ($\\ln(s)$) and growth-related factors ($\\ln(n + \\delta + g)$) are crucial for explaining variations in income per worker in the Solow Growth Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type 1 and Type 2 Errors in Hypothesis Testing\n",
    "\n",
    "When we perform hypothesis testing, we are making decisions based on sample data. However, there's always a chance of making an incorrect decision. These incorrect decisions are called **Type 1** and **Type 2** errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Type 1 Error\n",
    "\n",
    "- **Definition**: A Type 1 error occurs when we reject the null hypothesis ($H_0$) even though it is actually true.\n",
    "- **\"False Positive\"**: It's like sounding the fire alarm when there's no fire.\n",
    "- **Probability of Type 1 Error**: Denoted by **$\\alpha$** (significance level). Common choices for $\\alpha$ are 0.05 or 0.01.\n",
    "- **Impact**: Type 1 errors can lead to unnecessary actions or incorrect conclusions based on false findings.\n",
    "\n",
    "### Example:\n",
    "- Null Hypothesis ($H_0$): A new drug has no effect.\n",
    "- Type 1 Error: You conclude the drug works, but it actually doesn't.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Type 2 Error\n",
    "\n",
    "- **Definition**: A Type 2 error occurs when we fail to reject the null hypothesis ($H_0$) even though it is false.\n",
    "- **\"False Negative\"**: It's like ignoring the fire alarm when there is a fire.\n",
    "- **Probability of Type 2 Error**: Denoted by **$\\beta$**.\n",
    "- **Impact**: Type 2 errors can lead to missed opportunities or failure to detect real effects.\n",
    "\n",
    "### Example:\n",
    "- Null Hypothesis ($H_0$): A new drug has no effect.\n",
    "- Type 2 Error: You conclude the drug doesnâ€™t work, but it actually does.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "| **Type of Error** | **What Happens**                          | **Also Known As**     | **Probability** |\n",
    "|--------------------|------------------------------------------|------------------------|-----------------|\n",
    "| **Type 1 Error**   | Rejecting $H_0$ when itâ€™s true          | False Positive         | $\\alpha$        |\n",
    "| **Type 2 Error**   | Failing to reject $H_0$ when itâ€™s false | False Negative         | $\\beta$         |\n",
    "\n",
    "---\n",
    "\n",
    "## Reducing Errors\n",
    "\n",
    "1. **Reducing Type 1 Error**:\n",
    "   - Use a smaller significance level ($\\alpha$), e.g., 0.01 instead of 0.05.\n",
    "   - Be more cautious about rejecting the null hypothesis.\n",
    "\n",
    "2. **Reducing Type 2 Error**:\n",
    "   - Increase the sample size.\n",
    "   - Increase the power of the test (Power = $1 - \\beta$).\n",
    "   - Use a larger significance level ($\\alpha$), but this increases Type 1 Error.\n",
    "\n",
    "---\n",
    "\n",
    "## Trade-off Between Errors\n",
    "\n",
    "- Reducing Type 1 Error increases the chance of Type 2 Error, and vice versa.\n",
    "- The balance between the two depends on the context of the test:\n",
    "  - If the cost of a Type 1 Error is high (e.g., approving a harmful drug), keep $\\alpha$ low.\n",
    "  - If the cost of a Type 2 Error is high (e.g., missing a life-saving treatment), aim to reduce $\\beta$.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Testing in OLS Regression\n",
    "\n",
    "\n",
    "Diagnostic testing is essential to ensure that the assumptions underlying the Ordinary Least Squares (OLS) regression are valid. Before testing statistical hypotheses, it is crucial to verify the six fundamental assumptions of the classical linear regression model. Violating these assumptions affects the reliability of coefficient estimates, standard errors, and the validity of hypothesis testing.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions in Classical Linear Regression Model\n",
    "1. **Linearity (A1):** The relationship between independent variables and the dependent variable is linear.\n",
    "2. **Full Rank (A2):** The independent variables are not perfectly collinear.\n",
    "3. **Regression Model (A3):** The expected value of errors is zero, i.e., $ E(\\epsilon) = 0 $.\n",
    "4. **Spherical Errors (A4):** Errors have constant variance (homoscedasticity) and are not autocorrelated.\n",
    "5. **Non-Stochastic Regressors (A5):** The regressors are fixed and not random.\n",
    "6. **Normal Errors (A6):** Errors are normally distributed, $ \\epsilon \\sim N(0, \\sigma^2) $.\n",
    "\n",
    "---\n",
    "\n",
    "## Violations of Assumptions\n",
    "### Types of Violations\n",
    "1. **Functional Form Violations:** \n",
    "   - Incorrect model specification (e.g., nonlinearity, omitted variables).\n",
    "   - Example: Parameter instability or omitting relevant variables.\n",
    "\n",
    "2. **Regressor-Related Violations:**\n",
    "   - **Multicollinearity:** Independent variables are highly correlated (violates A2).\n",
    "   - **Stochastic Regressors:** Regressors are random (violates A5).\n",
    "\n",
    "3. **Error-Term Violations:**\n",
    "   - **Correlation with Regressors:** Errors are correlated with independent variables (violates A3).\n",
    "   - **Non-Spherical Errors:** Errors exhibit heteroskedasticity or autocorrelation (violates A4).\n",
    "   - **Non-Normal Errors:** Errors deviate from normality (violates A6).\n",
    "\n",
    "---\n",
    "\n",
    "## Consequences of Violations\n",
    "1. **Wrong Coefficient Estimates:** \n",
    "   - Violation of A3 leads to biased coefficients. Example: $ E(\\epsilon) \\neq 0 $, invalidating the t-test and hypothesis testing.\n",
    "\n",
    "2. **Incorrect Standard Errors:** \n",
    "   - Violation of A4 affects the efficiency of OLS estimates. The variance of the coefficients becomes unreliable, invalidating inference procedures.\n",
    "\n",
    "3. **Inappropriate Hypothesis Testing Distribution:** \n",
    "   - Violation of A6 implies that OLS coefficients and t-tests no longer follow their assumed distributions, making hypothesis tests invalid.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps to Handle Violations\n",
    "### Step 1: Detect Violations\n",
    "- Use diagnostic tests to identify violations. Example:\n",
    "  - F-tests or chi-square tests for model misspecifications.\n",
    "  - Residual analysis for detecting heteroskedasticity or autocorrelation.\n",
    "\n",
    "### Step 2: Analyze Consequences\n",
    "- Understand how violations affect:\n",
    "  - Coefficient estimates (bias).\n",
    "  - Standard errors (inefficiency).\n",
    "  - Distribution assumptions (invalid hypothesis tests).\n",
    "\n",
    "### Step 3: Address Violations\n",
    "1. **Eliminate the Violation:**\n",
    "   - Correct model specification (e.g., include omitted variables).\n",
    "   - Transform data to meet assumptions (e.g., log transformations).\n",
    "2. **Use Alternative Techniques:**\n",
    "   - **Instrumental Variable Estimation:** Address correlation between errors and regressors.\n",
    "   - **Generalized Least Squares (GLS):** Correct for heteroskedasticity and autocorrelation.\n",
    "\n",
    "---\n",
    "\n",
    "## Diagnostic Testing Procedure\n",
    "Diagnostic testing resembles hypothesis testing but focuses on detecting violations in model assumptions. This involves:\n",
    "1. Computing a test statistic based on residuals or model performance.\n",
    "2. Comparing the test statistic to a theoretical distribution (e.g., F-distribution or chi-square).\n",
    "3. Making adjustments if violations are detected to ensure the stability and credibility of the regression model.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "Diagnostic testing is a vital step in econometrics to ensure the robustness of OLS regression models. By systematically identifying and addressing violations, researchers can build credible models for reliable statistical inference and decision-making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  wlnyw   R-squared:                       0.199\n",
      "Model:                            OLS   Adj. R-squared:                  0.189\n",
      "Method:                 Least Squares   F-statistic:                     19.16\n",
      "Date:                Sat, 14 Dec 2024   Prob (F-statistic):           3.71e-08\n",
      "Time:                        13:38:18   Log-Likelihood:                -257.48\n",
      "No. Observations:                 157   AIC:                             521.0\n",
      "Df Residuals:                     154   BIC:                             530.1\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.2743      1.115      8.320      0.000       7.072      11.476\n",
      "lnwi           0.8482      0.222      3.829      0.000       0.411       1.286\n",
      "lnndg         -1.5861      0.407     -3.899      0.000      -2.390      -0.783\n",
      "==============================================================================\n",
      "Omnibus:                       10.520   Durbin-Watson:                   1.899\n",
      "Prob(Omnibus):                  0.005   Jarque-Bera (JB):                5.525\n",
      "Skew:                           0.256   Prob(JB):                       0.0631\n",
      "Kurtosis:                       2.237   Cond. No.                         42.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Variance Inflation Factor (VIF):\n",
      "  Variable         VIF\n",
      "0    const  122.957230\n",
      "1     lnwi    1.051117\n",
      "2    lnndg    1.051117\n",
      "\n",
      "Breusch-Pagan Test for Heteroskedasticity:\n",
      "LM Statistic: 0.8539989641353156, p-value: 0.6524638901599636\n",
      "\n",
      "Durbin-Watson Test for Autocorrelation:\n",
      "Durbin-Watson Statistic: 1.8986838170352356\n",
      "\n",
      "Jarque-Bera Test for Normality:\n",
      "JB Statistic: 5.524511494279639, p-value: 0.0631491590399429\n",
      "\n",
      "Ramsey RESET Test for Omitted Variables:\n",
      "F-statistic: 0.022207164494786207, p-value: 0.881733551563381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.diagnostic as smd\n",
    "from statsmodels.stats.stattools import jarque_bera, durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load the Solow model data\n",
    "file_path = \"data/solow_2000.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop NA values\n",
    "data = data.dropna()\n",
    "\n",
    "# Define the model variables\n",
    "y = data['wlnyw']  # Dependent variable\n",
    "X = data[['lnwi', 'lnndg']]  # Independent variables\n",
    "X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# 1. Test for Multicollinearity using VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Variable\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "print(\"\\nVariance Inflation Factor (VIF):\")\n",
    "print(vif_data)\n",
    "\n",
    "# 2. Test for Heteroskedasticity (Breusch-Pagan Test)\n",
    "bp_test = smd.het_breuschpagan(model.resid, model.model.exog)\n",
    "print(\"\\nBreusch-Pagan Test for Heteroskedasticity:\")\n",
    "print(f\"LM Statistic: {bp_test[0]}, p-value: {bp_test[1]}\")\n",
    "\n",
    "# 3. Test for Autocorrelation (Durbin-Watson Test)\n",
    "dw_stat = durbin_watson(model.resid)\n",
    "print(\"\\nDurbin-Watson Test for Autocorrelation:\")\n",
    "print(f\"Durbin-Watson Statistic: {dw_stat}\")\n",
    "\n",
    "# 4. Test for Normality of Errors (Jarque-Bera Test)\n",
    "jb_test = jarque_bera(model.resid)\n",
    "print(\"\\nJarque-Bera Test for Normality:\")\n",
    "print(f\"JB Statistic: {jb_test[0]}, p-value: {jb_test[1]}\")\n",
    "\n",
    "# 5. Test for Omitted Variables (Ramsey RESET Test)\n",
    "reset_test = smd.linear_reset(model, power=2, use_f=True)\n",
    "print(\"\\nRamsey RESET Test for Omitted Variables:\")\n",
    "print(f\"F-statistic: {reset_test.fvalue}, p-value: {reset_test.pvalue}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostic Test Results for the Solow Model\n",
    "\n",
    "### 1. **OLS Regression Results**\n",
    "- **Dependent Variable:** `wlnyw`\n",
    "- **R-squared:** 0.199  \n",
    "  - Approximately 19.9% of the variance in income per capita (`wlnyw`) is explained by the predictors.\n",
    "- **Adjusted R-squared:** 0.189  \n",
    "  - Adjusted for the number of predictors, slightly lower than R-squared.\n",
    "- **F-statistic:** 19.16, **p-value:** 3.71e-08  \n",
    "  - The model is statistically significant overall at conventional levels (p < 0.05), indicating that at least one predictor is useful.\n",
    "- **Coefficients:**\n",
    "  - **Intercept (`const`):** 9.2743  \n",
    "    - Significant (p < 0.001), representing the baseline `wlnyw` when `lnwi` and `lnndg` are zero.\n",
    "  - **`lnwi`:** 0.8482  \n",
    "    - Significant (p < 0.001), indicating a positive relationship between the savings rate and income per capita. For a 1-unit increase in `lnwi`, `wlnyw` increases by 0.8482 units on average.\n",
    "  - **`lnndg`:** -1.5861  \n",
    "    - Significant (p < 0.001), suggesting a negative relationship between the combined rate of population growth, depreciation, and technological growth (`lnndg`) and income per capita. A 1-unit increase in `lnndg` reduces `wlnyw` by 1.5861 units on average.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Variance Inflation Factor (VIF)**\n",
    "- **Purpose:** Detects multicollinearity among predictors.\n",
    "- **Results:**\n",
    "  - **`const`:** 122.96  \n",
    "    - This high value is common for intercepts and not a concern.\n",
    "  - **`lnwi`:** 1.05  \n",
    "  - **`lnndg`:** 1.05  \n",
    "    - Both VIFs are low (< 10), indicating no multicollinearity issues among predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Breusch-Pagan Test for Heteroskedasticity**\n",
    "- **LM Statistic:** 0.854, **p-value:** 0.652  \n",
    "  - The null hypothesis of homoscedasticity (constant error variance) is not rejected (p > 0.05). This suggests no evidence of heteroskedasticity.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Durbin-Watson Test for Autocorrelation**\n",
    "- **Durbin-Watson Statistic:** 1.899  \n",
    "  - Values close to 2 indicate no autocorrelation in residuals. This result suggests no significant autocorrelation.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Jarque-Bera Test for Normality**\n",
    "- **JB Statistic:** 5.524, **p-value:** 0.063  \n",
    "  - The null hypothesis of normally distributed residuals is not rejected (p > 0.05). Residuals are approximately normal.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Ramsey RESET Test for Omitted Variables**\n",
    "- **F-statistic:** 0.022, **p-value:** 0.882  \n",
    "  - The null hypothesis of no omitted variable bias is not rejected (p > 0.05). There is no strong evidence of model misspecification due to omitted variables.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Diagnostic Test Results\n",
    "- **Multicollinearity:** No issues; VIF values are low.\n",
    "- **Heteroskedasticity:** No evidence; residual variance is constant.\n",
    "- **Autocorrelation:** None detected; residuals are not correlated.\n",
    "- **Normality:** Residuals are approximately normal.\n",
    "- **Model Specification:** No evidence of omitted variable bias.\n",
    "\n",
    "### Conclusion\n",
    "The diagnostic tests confirm that the assumptions of the classical linear regression model are satisfied for the Solow growth model data. The model is statistically sound and valid for inference. The predictors (`lnwi` and `lnndg`) are meaningful and have a statistically significant impact on `wlnyw`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Violations of Assumptions in Regression Analysis\n",
    "\n",
    "## Violations of Full Rank\n",
    "- **Definition:** Occurs when regressors are highly correlated but not perfectly linearly correlated. Perfect multicollinearity prevents running an OLS model, but near multicollinearity makes parameter estimation difficult.\n",
    "- **Assumption Violated:** Full rank of the matrix (A2).\n",
    "  - This assumes the matrix has no perfect linear combinations of columns.\n",
    "  - Example: If $x_3 = 2x_2$, $\\beta_3$ and $\\beta_2$ cannot be identified separately.\n",
    "- **Consequences:**\n",
    "  - High $R^2$ but high standard errors for coefficients.\n",
    "  - Regression results are sensitive to small specification changes.\n",
    "  - Confidence intervals become very wide, leading to unreliable significance tests.\n",
    "- **Solution:**\n",
    "  - Check correlation matrices for high correlations among regressors.\n",
    "  - Remove one of the collinear variables.\n",
    "  - Transform variables into ratios.\n",
    "  - Collect more data (e.g., longer or higher-frequency data).\n",
    "\n",
    "---\n",
    "\n",
    "## Test for the Violation of Linearity\n",
    "\n",
    "### 1. **Non-Linear Model: Ramsey RESET Test**\n",
    "- **Purpose:** Detects incorrect functional forms (non-linearity).\n",
    "- **Procedure:**\n",
    "  1. Collect residuals from the original model.\n",
    "  2. Regress residuals against a constant and higher-order fitted values of $y$ (e.g., $y^2$, $y^3$).\n",
    "  3. Compute $R^2$ from this auxiliary regression.\n",
    "  4. Test statistic: $R^2T \\sim \\chi^2_{p-1}$.\n",
    "- **Interpretation:** If the test statistic exceeds the critical $\\chi^2$ value, reject the null hypothesis of correct functional form.\n",
    "\n",
    "### 2. **Structural Break: Chow Test**\n",
    "- **Purpose:** Detects parameter instability over time.\n",
    "- **Procedure:**\n",
    "  1. Split the data into two sub-samples.\n",
    "  2. Estimate separate regressions for each sub-sample.\n",
    "  3. Compute residual sum of squares (RSS) for the full sample ($RSS$) and sub-samples ($RSS_1$, $RSS_2$).\n",
    "  4. Test statistic: \n",
    "     $$\n",
    "     \\text{Chow Test Statistic} = \\frac{RSS - (RSS_1 + RSS_2)}{RSS_1 + RSS_2} \\cdot \\frac{T - 2k}{k}\n",
    "     $$\n",
    "     where $T$ is the number of observations and $k$ is the number of regressors.\n",
    "- **Interpretation:** If the statistic is larger than the critical F-value, reject the null hypothesis of parameter stability.\n",
    "\n",
    "- **Problems with Chow Test:**\n",
    "  - Requires enough data for sub-sample regressions.\n",
    "  - Arbitrary choice of split point unless prior knowledge exists.\n",
    "\n",
    "---\n",
    "\n",
    "## Predictive Failure Test\n",
    "- **Purpose:** Checks stability of regression relationships by forecasting or back-casting.\n",
    "- **Types:**\n",
    "  1. **Forward Test:** Uses the last few observations for forecast testing.\n",
    "  2. **Backward Test:** Attempts to \"back-cast\" the first few observations.\n",
    "- **Procedure:**\n",
    "  1. Run the regression on the full period and compute $RSS$ (restricted).\n",
    "  2. Run the regression on the \"large\" sub-period and compute $RSS_1$.\n",
    "  3. Test statistic:\n",
    "     $$\n",
    "     \\text{Test Statistic} = \\frac{RSS - RSS_1}{RSS_1} \\cdot \\frac{T_1 - k}{T_2}\n",
    "     $$\n",
    "     where $T_1$ is the number of observations used in estimation, and $T_2$ is the number used for prediction.\n",
    "  4. Follows $F(T_2, T_1 - k)$ distribution.\n",
    "- **Interpretation:** If the test statistic exceeds the critical value, reject the null hypothesis of parameter stability.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "### Common Tests for Violations:\n",
    "1. **Ramsey RESET Test**: Detects incorrect functional forms.\n",
    "2. **Chow Test**: Identifies structural breaks and parameter instability.\n",
    "3. **Predictive Failure Test**: Verifies stability of regression relationships.\n",
    "\n",
    "### Solutions to Violations:\n",
    "- For multicollinearity:\n",
    "  - Drop collinear variables or collect more data.\n",
    "- For non-linearity:\n",
    "  - Use Ramsey RESET or non-linear regression techniques.\n",
    "- For structural breaks:\n",
    "  - Apply Chow Test and split data appropriately.\n",
    "\n",
    "Understanding and addressing these violations ensures robust and reliable regression analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  wlnyw   R-squared:                       0.199\n",
      "Model:                            OLS   Adj. R-squared:                  0.189\n",
      "Method:                 Least Squares   F-statistic:                     19.16\n",
      "Date:                Sat, 14 Dec 2024   Prob (F-statistic):           3.71e-08\n",
      "Time:                        13:47:58   Log-Likelihood:                -257.48\n",
      "No. Observations:                 157   AIC:                             521.0\n",
      "Df Residuals:                     154   BIC:                             530.1\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.2743      1.115      8.320      0.000       7.072      11.476\n",
      "lnwi           0.8482      0.222      3.829      0.000       0.411       1.286\n",
      "lnndg         -1.5861      0.407     -3.899      0.000      -2.390      -0.783\n",
      "==============================================================================\n",
      "Omnibus:                       10.520   Durbin-Watson:                   1.899\n",
      "Prob(Omnibus):                  0.005   Jarque-Bera (JB):                5.525\n",
      "Skew:                           0.256   Prob(JB):                       0.0631\n",
      "Kurtosis:                       2.237   Cond. No.                         42.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Ramsey RESET Test for Linearity:\n",
      "F-statistic: 0.022207164494786207, p-value: 0.881733551563381\n",
      "\n",
      "Chow Test for Structural Break:\n",
      "Chow Test Statistic: 0.2101191159905775, p-value: 0.8892722453265062\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import linear_reset\n",
    "from scipy.stats import f  # Import F-distribution from scipy.stats\n",
    "\n",
    "# Load the Solow model data\n",
    "file_path = \"data/solow_2000.csv\"  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop NA values\n",
    "data = data.dropna()\n",
    "\n",
    "# Define the model variables\n",
    "y = data['wlnyw']  # Dependent variable\n",
    "X = data[['lnwi', 'lnndg']]  # Independent variables\n",
    "X = sm.add_constant(X)  # Add a constant for the intercept\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# 1. Ramsey RESET Test for Linearity\n",
    "print(\"\\nRamsey RESET Test for Linearity:\")\n",
    "reset_test = linear_reset(model, power=2, use_f=True)\n",
    "print(f\"F-statistic: {reset_test.fvalue}, p-value: {reset_test.pvalue}\")\n",
    "\n",
    "# 2. Chow Test for Structural Break\n",
    "def chow_test(data, breakpoint, dependent_var, independent_vars):\n",
    "    \"\"\"Perform a Chow Test to check for structural breaks at a given breakpoint.\"\"\"\n",
    "    # Split the data\n",
    "    data1 = data.iloc[:breakpoint]\n",
    "    data2 = data.iloc[breakpoint:]\n",
    "\n",
    "    # Define variables for sub-sample 1\n",
    "    y1 = data1[dependent_var]\n",
    "    X1 = data1[independent_vars]\n",
    "    X1 = sm.add_constant(X1)\n",
    "\n",
    "    # Define variables for sub-sample 2\n",
    "    y2 = data2[dependent_var]\n",
    "    X2 = data2[independent_vars]\n",
    "    X2 = sm.add_constant(X2)\n",
    "\n",
    "    # Fit models for each sub-sample\n",
    "    model1 = sm.OLS(y1, X1).fit()\n",
    "    model2 = sm.OLS(y2, X2).fit()\n",
    "\n",
    "    # RSS for each sub-sample\n",
    "    RSS1 = np.sum(model1.resid ** 2)\n",
    "    RSS2 = np.sum(model2.resid ** 2)\n",
    "\n",
    "    # RSS for the full sample\n",
    "    RSS_full = np.sum(model.resid ** 2)\n",
    "\n",
    "    # Number of observations and parameters\n",
    "    T = len(data)\n",
    "    k = len(X.columns)\n",
    "    T1 = len(data1)\n",
    "    T2 = len(data2)\n",
    "\n",
    "    # Chow Test statistic\n",
    "    chow_stat = ((RSS_full - (RSS1 + RSS2)) / k) / ((RSS1 + RSS2) / (T - 2 * k))\n",
    "    p_value = 1 - f.cdf(chow_stat, k, T - 2 * k)  # Use scipy.stats.f.cdf\n",
    "\n",
    "    return chow_stat, p_value\n",
    "\n",
    "# Perform the Chow Test\n",
    "breakpoint = len(data) // 2  # Split data into two equal halves\n",
    "chow_stat, chow_p_value = chow_test(data, breakpoint, 'wlnyw', ['lnwi', 'lnndg'])\n",
    "\n",
    "print(\"\\nChow Test for Structural Break:\")\n",
    "print(f\"Chow Test Statistic: {chow_stat}, p-value: {chow_p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of Diagnostic Test Results\n",
    "\n",
    "## 1. Ramsey RESET Test for Linearity\n",
    "The **Ramsey RESET Test** evaluates whether the model has the correct functional form (i.e., whether the relationship between the dependent variable and the independent variables is linear). Here's what the results indicate:\n",
    "\n",
    "- **F-statistic**: 0.0222\n",
    "- **p-value**: 0.8817\n",
    "\n",
    "### Interpretation:\n",
    "- A **high p-value** (greater than 0.05) indicates that we fail to reject the null hypothesis, which assumes the model has the correct functional form.\n",
    "- In this case, the p-value of 0.8817 suggests that the model does not show evidence of non-linearity. Therefore, the functional form of the Solow growth model appears appropriate.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Chow Test for Structural Break\n",
    "The **Chow Test** checks for structural breaks in the data, i.e., whether the relationship between variables changes across different sub-samples. The test splits the data into two halves and compares the parameter stability.\n",
    "\n",
    "- **Chow Test Statistic**: 0.2101\n",
    "- **p-value**: 0.8893\n",
    "\n",
    "### Interpretation:\n",
    "- A **high p-value** (greater than 0.05) means we fail to reject the null hypothesis, which assumes that the parameters are stable across the two sub-samples.\n",
    "- In this case, the p-value of 0.8893 indicates that there is no significant evidence of a structural break in the model. The relationships between the variables remain consistent over the two sub-samples.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "- **Ramsey RESET Test**: The model's functional form is correctly specified, and no evidence of non-linearity is found.\n",
    "- **Chow Test**: The model does not exhibit structural instability, and the parameters are stable over time.\n",
    "\n",
    "### Implications\n",
    "These diagnostic tests confirm that the Solow growth model does not have significant issues with functional form or parameter instability, making it a reliable framework for interpreting the relationship between the dependent and independent variables. However, further diagnostic tests (e.g., for multicollinearity or heteroskedasticity) should be considered for a comprehensive evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Violation of Assumptions in Regression Models\n",
    "\n",
    "### Key Assumptions and Their Violations\n",
    "In regression models, the assumptions ensure the model's validity. Violating these assumptions can lead to biased, inefficient, or inconsistent results. Below, we discuss three main types of violations:\n",
    "\n",
    "## 1. Omitted Variables\n",
    "- **Definition**: When a relevant variable is excluded from the model, its effect is absorbed into the error term, causing correlation between the error term and regressors.\n",
    "- **Example**: Estimating a consumption function without including wealth as a variable.\n",
    "- **Effect**: The coefficient estimates become biased, as the omitted variable's effect is incorrectly attributed to included regressors.\n",
    "- **Correction**: Include the missing variable if possible or use **Instrumental Variables (IV)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Simultaneity\n",
    "- **Definition**: When a regressor is endogenous (dependent on the dependent variable), there is a bidirectional relationship between the dependent and independent variables.\n",
    "- **Example**: Consumption function where consumption ($C_t$) depends on income ($Y_t$), and income depends on consumption.\n",
    "- **Effect**: Correlation between regressors and error terms leads to biased and inconsistent OLS estimates.\n",
    "- **Correction**: Use **Two-Stage Least Squares (2SLS)** or **IV** estimators.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Measurement Error\n",
    "- **Definition**: Occurs when variables are measured with error, causing misrepresentation in the model.\n",
    "- **Example**: Using a proxy ($Z$) instead of the true value of a variable ($X$).\n",
    "- **Effect**: Leads to biased, inefficient, and inconsistent estimates.\n",
    "- **Correction**: Use **Generalized Method of Moments (GMM)**, **2SLS**, or robust instruments.\n",
    "\n",
    "---\n",
    "\n",
    "### Testing for Violations\n",
    "#### Hausman Test for Exogeneity\n",
    "- **Purpose**: Checks for correlation between regressors and error terms.\n",
    "- **Null Hypothesis ($H_0$)**: Regressors are exogenous (not correlated with the error term).\n",
    "- **Alternative Hypothesis ($H_A$)**: Regressors are endogenous.\n",
    "- **Procedure**:\n",
    "  1. Estimate the model using both OLS and IV.\n",
    "  2. Calculate the \"distance\" between OLS and IV estimators.\n",
    "  3. Test statistic:\n",
    "     $$ H = n^{1/2}(\\hat{\\beta}_{IV} - \\hat{\\beta}_{OLS})^{\\prime} \\left[\\widehat{Var}(n^{1/2}(\\hat{\\beta}_{IV} - \\hat{\\beta}_{OLS}))\\right]^{-1} n^{1/2}(\\hat{\\beta}_{IV} - \\hat{\\beta}_{OLS}) $$\n",
    "- **Outcome**:\n",
    "  - If $H \\to \\chi^2_k$, fail to reject $H_0$ (OLS is consistent).\n",
    "  - If $H$ diverges, reject $H_0$ (OLS is inconsistent, prefer IV).\n",
    "\n",
    "---\n",
    "\n",
    "### Consequences of Violations\n",
    "1. **Biased Coefficients**:\n",
    "   - Occurs when regressors are correlated with errors (e.g., omitted variables, simultaneity).\n",
    "   - Leads to incorrect inferences.\n",
    "2. **Inefficient Estimators**:\n",
    "   - Violations like heteroskedasticity affect standard errors, making estimators inefficient.\n",
    "3. **Invalid Hypothesis Testing**:\n",
    "   - Non-normal errors or correlated regressors lead to invalid test results.\n",
    "\n",
    "---\n",
    "\n",
    "### Solutions to Violations\n",
    "1. **Omitted Variables**:\n",
    "   - Include the missing variable.\n",
    "   - Use IV for unbiased estimation.\n",
    "2. **Simultaneity**:\n",
    "   - Employ 2SLS or GMM estimators.\n",
    "3. **Measurement Error**:\n",
    "   - Use robust instruments or external variables as proxies.\n",
    "4. **General Approach**:\n",
    "   - Diagnostic testing (e.g., Hausman test) to identify the issue.\n",
    "   - Apply alternative estimation techniques if violations persist.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "Regression assumptions are foundational for valid statistical inferences. Violations like omitted variables, simultaneity, and measurement error can bias or invalidate results. Tests like the Hausman test help detect issues, while methods like IV and 2SLS provide robust solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consequences of the Violations\n",
    "\n",
    "> \"We assumed that the variance of the error terms is constant. This condition is called conditional homoscedasticity, while its violation is called conditional heteroscedasticity.\"\n",
    "\n",
    "## Heteroscedasticity Explained\n",
    "- **Definition:** If heteroscedasticity occurs, the variance of the error term for one observation differs from another. This is common in cross-sectional data.\n",
    "- **Reason:** There is no inherent reason to assume the error variance for one individual equals anotherâ€™s due to uncorrelated behaviors.\n",
    "- **Conditional:** It depends on the explanatory variables ($ X $).\n",
    "- **Consequences:** The variance of $ \\hat{\\beta} $ is no longer efficient, affecting inference:\n",
    "  - Variance of $ \\hat{\\beta} $ includes $ \\sigma^2 (X'X)^{-1} $.\n",
    "  - Affects the denominator of the $ t $-test formula.\n",
    "  - $ \\hat{\\beta} $ remains unbiased ($ E(\\hat{\\beta}) = \\beta $) but is no longer BLUE (Best Linear Unbiased Estimator).\n",
    "\n",
    "---\n",
    "\n",
    "# Tests for Violations\n",
    "\n",
    "## Goldfeld and Quandt (GQ) Test\n",
    "- **Method:**\n",
    "  1. Split the sample into two groups.\n",
    "  2. Test whether the variances of the two groups are equal:\n",
    "     - Null Hypothesis ($ H_0 $): $ \\sigma_1^2 = \\sigma_2^2 $\n",
    "     - Alternative Hypothesis ($ H_A $): $ \\sigma_1^2 \\neq \\sigma_2^2 $\n",
    "  3. Expressed as a ratio:\n",
    "     - $ H_0: \\frac{\\sigma_1^2}{\\sigma_2^2} = 1 $\n",
    "     - $ H_A: \\frac{\\sigma_1^2}{\\sigma_2^2} \\neq 1 $\n",
    "- **Test Statistic:**\n",
    "  - Compute $ s_1^2 $ and $ s_2^2 $ from residuals:\n",
    "    - $ s_1^2 = \\frac{\\epsilon' \\epsilon}{T_1 - k} $\n",
    "    - $ s_2^2 = \\frac{\\epsilon' \\epsilon}{T_2 - k} $\n",
    "  - Larger variance is placed in the numerator for an F-distribution test:\n",
    "    - $ F \\sim F_{T_1 - k, T_2 - k} $\n",
    "- **Decision:**\n",
    "  - Reject $ H_0 $ if $ F $ exceeds the critical value, indicating heteroscedasticity.\n",
    "\n",
    "> \"Operatively, the Goldfeld and Quandt test is simply the ratio between the two variances.\"\n",
    "\n",
    "### Drawbacks\n",
    "- Requires splitting the sample, which can be arbitrary.\n",
    "- Best suited for unconditional heteroscedasticity.\n",
    "\n",
    "---\n",
    "\n",
    "## White Test\n",
    "- **Purpose:** Detects heteroscedasticity by examining whether the error term variance depends on the regressors.\n",
    "- **Auxiliary Regression:**\n",
    "  - Regress squared residuals ($ \\hat{\\epsilon}_i^2 $) on the regressors, their squares, and interaction terms:\n",
    "    - $ \\hat{\\epsilon}_i^2 = \\alpha_1 + \\alpha_2 x_{2i} + \\alpha_3 x_{3i} + \\alpha_4 x_{2i}^2 + \\alpha_5 x_{3i}^2 + \\alpha_6 x_{2i}x_{3i} + \\nu_i $\n",
    "  - Includes intercept to account for non-linear relations and cross-products.\n",
    "- **Null Hypothesis ($ H_0 $):**\n",
    "  - $ \\alpha_1 = \\alpha_2 = \\alpha_3 = \\alpha_4 = \\alpha_5 = \\alpha_6 = 0 $ (homoscedasticity).\n",
    "- **Test Statistic:**\n",
    "  - $ TR^2 \\sim \\chi^2_m $, where $ m $ is the number of regressors excluding the constant term.\n",
    "  - Reject $ H_0 $ if $ TR^2 $ exceeds the critical value from the chi-square table.\n",
    "\n",
    "> \"Notice that the auxiliary regression must always contain the intercept.\"\n",
    "\n",
    "### Interpretation\n",
    "- High $ R^2 $ indicates that the regressors explain the variance, leading to rejection of $ H_0 $.\n",
    "- Ideally, all $ \\alpha $'s (except the intercept) should be insignificant for homoscedasticity.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "- **Heteroscedasticity:**\n",
    "  - Affects variance but not the unbiased nature of $ \\hat{\\beta} $.\n",
    "  - Practical implications for statistical inference.\n",
    "- **Tests:**\n",
    "  - GQ Test is simpler but has limitations due to sample splitting.\n",
    "  - White Test is more robust, especially for conditional heteroscedasticity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violation of Autocorrelation\n",
    "\n",
    "## **What is Autocorrelation?**\n",
    "- Autocorrelation occurs when error terms ($ \\epsilon_t $) are correlated with each other across time.\n",
    "- **Example:** If todayâ€™s errors are positive, tomorrowâ€™s errors are also likely to be positive (positive autocorrelation) or negative (negative autocorrelation).\n",
    "- It commonly happens in **time series data** because the same individual is studied over time.\n",
    "\n",
    "> \"Autocorrelation is more likely to happen while dealing with time series data, when we fix the individual and study their behavior over time.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Consequences of Autocorrelation**\n",
    "- Autocorrelation violates the assumption of uncorrelated error terms.\n",
    "- **Impacts on OLS:**\n",
    "  - The OLS estimator is **no longer BLUE** (Best Linear Unbiased Estimator).\n",
    "  - Variance of the estimator is incorrect:\n",
    "    - In **static models**: Variance is underestimated, making the estimator inefficient.\n",
    "    - In **dynamic models**: The estimator becomes inconsistent.\n",
    "  - Standard error estimates are biased, leading to incorrect inferences:\n",
    "    - Increased likelihood of **Type I error** (rejecting a true null hypothesis).\n",
    "    - $ R^2 $ is inflated, overstating the model's fit.\n",
    "- **Example:** If seasonality is not modeled, it can appear as autocorrelation in the residuals.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tests for Autocorrelation**\n",
    "\n",
    "### **1. Durbin-Watson (DW) Test**\n",
    "- **Purpose:** Detects **first-order autocorrelation** (correlation between $ \\epsilon_t $ and $ \\epsilon_{t-1} $).\n",
    "- **Model:** $ \\epsilon_t = \\rho \\epsilon_{t-1} + \\nu_t $\n",
    "- **Null Hypothesis ($ H_0 $):** $ \\rho = 0 $ (no autocorrelation).\n",
    "- **Test Statistic:**\n",
    "  - $ DW = \\frac{\\sum_{t=2}^T (\\hat{\\epsilon}_t - \\hat{\\epsilon}_{t-1})^2}{\\sum_{t=1}^T \\hat{\\epsilon}_t^2} $\n",
    "  - $ DW \\approx 2(1 - \\hat{\\rho}) $\n",
    "- **Decision Rule:**\n",
    "  - $ DW = 2 $: No autocorrelation ($ \\rho = 0 $).\n",
    "  - $ DW = 0 $: Perfect positive autocorrelation ($ \\rho = 1 $).\n",
    "  - $ DW = 4 $: Perfect negative autocorrelation ($ \\rho = -1 $).\n",
    "  - Use critical values ($ d_L $ and $ d_U $):\n",
    "    - Reject $ H_0 $ if $ DW < d_L $ (positive autocorrelation).\n",
    "    - Reject $ H_0 $ if $ DW > 4 - d_L $ (negative autocorrelation).\n",
    "    - No conclusion if $ d_L < DW < d_U $.\n",
    "\n",
    "#### **Drawbacks:**\n",
    "- Limited to detecting first-order autocorrelation.\n",
    "- Inconclusive when $ DW $ lies between $ d_L $ and $ d_U $.\n",
    "- Cannot handle dynamic models.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Breusch-Godfrey (BG) Test**\n",
    "- **Purpose:** Detects higher-order autocorrelation.\n",
    "- **Steps:**\n",
    "  1. **Estimate the original regression model** and compute residuals.\n",
    "  2. **Auxiliary regression:** Regress residuals on original regressors plus lagged residuals ($ \\hat{\\epsilon}_{t-1}, \\hat{\\epsilon}_{t-2}, \\ldots, \\hat{\\epsilon}_{t-r} $).\n",
    "  3. **Null Hypothesis ($ H_0 $):** $ \\rho_1 = \\rho_2 = \\ldots = \\rho_r = 0 $ (no autocorrelation).\n",
    "  4. Calculate test statistic:\n",
    "     - $ BG = (T - r) R^2 \\sim \\chi^2_r $, where $ r $ is the number of lags.\n",
    "- **Decision Rule:**\n",
    "  - Reject $ H_0 $ if $ BG $ exceeds the critical value from the Chi-squared table.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of Tests**\n",
    "\n",
    "| **Test**               | **Purpose**                        | **Strengths**                        | **Weaknesses**                              |\n",
    "|------------------------|------------------------------------|--------------------------------------|--------------------------------------------|\n",
    "| **Durbin-Watson**      | Detects first-order autocorrelation | Simple and widely used               | Limited to first-order autocorrelation; inconclusive zones. |\n",
    "| **Breusch-Godfrey**    | Detects higher-order autocorrelation | Handles multiple lags and dynamic models | Requires more computational effort.         |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "- Autocorrelation in residuals often arises when important dynamics (e.g., seasonality or lags) are not modeled.\n",
    "- It makes OLS estimates inefficient, biased, and unreliable for statistical inference.\n",
    "- Use **Durbin-Watson** for first-order tests and **Breusch-Godfrey** for higher-order autocorrelation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Regressors in OLS\n",
    "\n",
    "## **What Are Stochastic Regressors?**\n",
    "- **Non-Stochastic Regressors:** Assumes regressors ($ X $) are fixed and do not change if we change the sample.\n",
    "  - True in experimental data where regressors are under our control.\n",
    "- **Stochastic Regressors:** When $ X $ is randomly drawn from a probability distribution, it changes as the sample changes.\n",
    "  - Common in economics and finance where both $ X $ and $ Y $ are stochastic.\n",
    "\n",
    "---\n",
    "\n",
    "## **Analyzing OLS with Stochastic Regressors**\n",
    "To understand the properties of the OLS estimator ($ \\hat{\\beta} $):\n",
    "1. **Conditional Analysis:** Analyze $ \\hat{\\beta} $ given $ X $ (like in the non-stochastic case).\n",
    "2. **Expectation Over Distribution:** Average results over the distribution of $ X $.\n",
    "\n",
    "---\n",
    "\n",
    "## **Consequences of Stochastic Regressors**\n",
    "- Even with stochastic regressors, $ \\hat{\\beta} $ remains **unbiased** for $ \\beta $.\n",
    "- Variance of $ \\hat{\\beta} $ is **different** because it depends on the expectation of $ (X'X)^{-1} $:\n",
    "  $$\n",
    "  \\text{Var}(\\hat{\\beta}) = \\sigma^2 E\\left[(X'X)^{-1}\\right]\n",
    "  $$\n",
    "  - Unlike non-stochastic regressors, where variance is a fixed number, here it is an expectation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Results**\n",
    "- If assumptions $ A1 $ (linearity), $ A2 $ (full rank), $ A3 $ (correct model), and $ A4 $ (spherical errors) hold, but **$ A5 $ (non-stochastic regressors)** does not:\n",
    "  - $ \\hat{\\beta} $ is still **unbiased**:\n",
    "    $$\n",
    "    E(\\hat{\\beta}) = \\beta\n",
    "    $$\n",
    "  - Variance of $ \\hat{\\beta} $:\n",
    "    $$\n",
    "    \\text{Var}(\\hat{\\beta}) = \\sigma^2 E\\left[(X'X)^{-1}\\right]\n",
    "    $$\n",
    "  - The **Gauss-Markov theorem** still holds: $ \\hat{\\beta} $ is the **linear unbiased estimator with minimum variance**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Implications**\n",
    "- With stochastic regressors, the variance is estimated as an expected value instead of a fixed number.\n",
    "- The OLS properties still hold, making $ \\hat{\\beta} $ the best available estimator for $ \\beta $.\n",
    "- Retaining the assumption of non-stochastic regressors simplifies presentations and interpretations but is often unrealistic in real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "> **Quote:** \"Our non-stochastic regressors assumption states that the regressors do not change even if we change the sample size.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Insights**\n",
    "### **Unbiasedness**\n",
    "Using the law of iterated expectations:\n",
    "$$\n",
    "E(\\hat{\\beta} | X) = \\beta\n",
    "$$\n",
    "$$\n",
    "E(\\hat{\\beta}) = E_X(E(\\hat{\\beta} | X)) = \\beta\n",
    "$$\n",
    "This shows that $ \\hat{\\beta} $ is unbiased even with stochastic $ X $.\n",
    "\n",
    "### **Variance**\n",
    "The variance of $ \\hat{\\beta} $ with stochastic $ X $:\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = E_X\\left[\\text{Var}(\\hat{\\beta} | X)\\right] + \\text{Var}_X\\left[E(\\hat{\\beta} | X)\\right]\n",
    "$$\n",
    "If $ A1, A2, A3, $ and $ A4 $ hold, but $ A5 $ does not, the variance becomes:\n",
    "$$\n",
    "\\text{Var}(\\hat{\\beta}) = \\sigma^2 E\\left[(X'X)^{-1}\\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "- While the assumption of non-stochastic regressors simplifies theory, it is often unrealistic in practical applications like economics and finance.\n",
    "- With stochastic regressors, $ \\hat{\\beta} $ retains its key properties (unbiasedness, minimum variance) under the Gauss-Markov theorem.\n",
    "- The main difference lies in the way variance is calculated and interpreted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  lnffr   R-squared:                       0.328\n",
      "Model:                            OLS   Adj. R-squared:                  0.322\n",
      "Method:                 Least Squares   F-statistic:                     54.19\n",
      "Date:                Sat, 14 Dec 2024   Prob (F-statistic):           6.84e-20\n",
      "Time:                        14:26:07   Log-Likelihood:                -382.92\n",
      "No. Observations:                 225   AIC:                             771.8\n",
      "Df Residuals:                     222   BIC:                             782.1\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.2298      0.155     -1.484      0.139      -0.535       0.075\n",
      "inflation      0.3193      0.034      9.517      0.000       0.253       0.385\n",
      "bus_cycle      0.1564      0.057      2.738      0.007       0.044       0.269\n",
      "==============================================================================\n",
      "Omnibus:                       33.448   Durbin-Watson:                   0.108\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               43.694\n",
      "Skew:                          -1.057   Prob(JB):                     3.25e-10\n",
      "Kurtosis:                       3.442   Cond. No.                         8.34\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durbin-Watson Statistic: 0.10766206806573425\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import durbin_watson  # Corrected import\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(\"data/taylor.xlsx\")\n",
    "\n",
    "# Estimate the model lnffr ~ inflation + bus_cycle\n",
    "# Add a constant to the independent variables\n",
    "X = sm.add_constant(df[['inflation', 'bus_cycle']])\n",
    "y = df['lnffr']\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(model.summary())\n",
    "\n",
    "# Save residuals\n",
    "residuals = model.resid\n",
    "\n",
    "# Test for autocorrelation in residuals using ACF and PACF\n",
    "# Plot ACF and PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sm.graphics.tsa.plot_acf(residuals, ax=axes[0], title='ACF of Residuals')\n",
    "sm.graphics.tsa.plot_pacf(residuals, ax=axes[1], title='PACF of Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report the Durbin-Watson statistic\n",
    "dw_stat = durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson Statistic: {dw_stat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of OLS Regression Results and Residual Diagnostics\n",
    "\n",
    "## **OLS Regression Results**\n",
    "\n",
    "### **Model Overview**\n",
    "- **Dependent Variable:** `lnffr`\n",
    "- **Independent Variables:** `inflation`, `bus_cycle`\n",
    "- **Number of Observations:** 225\n",
    "- **R-squared:** 0.328\n",
    "  - About 32.8% of the variability in `lnffr` is explained by `inflation` and `bus_cycle`.\n",
    "- **Adjusted R-squared:** 0.322\n",
    "  - Adjusted for the number of predictors; slightly lower than R-squared.\n",
    "\n",
    "### **Model Coefficients**\n",
    "| Variable   | Coefficient | Std. Error | t-value | p-value | 95% Confidence Interval |\n",
    "|------------|-------------|------------|---------|---------|--------------------------|\n",
    "| Constant   | -0.2298     | 0.155      | -1.484  | 0.139   | [-0.535, 0.075]         |\n",
    "| Inflation  | 0.3193      | 0.034      | 9.517   | 0.000   | [0.253, 0.385]          |\n",
    "| Bus Cycle  | 0.1564      | 0.057      | 2.738   | 0.007   | [0.044, 0.269]          |\n",
    "\n",
    "#### **Interpretation of Coefficients:**\n",
    "1. **Inflation:** A unit increase in inflation is associated with a 0.3193 unit increase in `lnffr`, holding all else constant. This relationship is highly significant ($ p < 0.001 $).\n",
    "2. **Bus Cycle:** A unit increase in `bus_cycle` is associated with a 0.1564 unit increase in `lnffr`. This relationship is also statistically significant ($ p = 0.007 $).\n",
    "\n",
    "### **Model Fit**\n",
    "- **F-statistic:** 54.19 ($ p < 0.001 $)\n",
    "  - Indicates the model is statistically significant overall.\n",
    "- **AIC (771.8) and BIC (782.1):**\n",
    "  - Metrics to compare model fit, lower values indicate better fit.\n",
    "\n",
    "### **Residual Diagnostics**\n",
    "- **Durbin-Watson Statistic:** 0.108\n",
    "  - Indicates strong **positive autocorrelation** in residuals, which violates the OLS assumption of no autocorrelation.\n",
    "- **Omnibus and Jarque-Bera Tests:**\n",
    "  - Both tests reject the null hypothesis of normally distributed residuals ($ p < 0.001 $).\n",
    "  - The residuals exhibit skewness (-1.057) and non-normality.\n",
    "\n",
    "---\n",
    "\n",
    "## **Residual Analysis with ACF and PACF**\n",
    "\n",
    "### **Autocorrelation (ACF) Plot**\n",
    "- The ACF plot shows a gradual decline in autocorrelation values over lags, indicating significant **positive autocorrelation** in residuals.\n",
    "- Residual autocorrelations are outside the 95% confidence bounds for several lags.\n",
    "\n",
    "### **Partial Autocorrelation (PACF) Plot**\n",
    "- The PACF plot shows a significant spike at lag 1, confirming **first-order autocorrelation** in residuals.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Issues Identified**\n",
    "1. **Autocorrelation in Residuals:**\n",
    "   - The Durbin-Watson statistic (0.108) and ACF/PACF plots confirm significant positive autocorrelation.\n",
    "   - Positive autocorrelation violates the OLS assumption of independent residuals, leading to:\n",
    "     - Biased standard errors.\n",
    "     - Inflated $ R^2 $ values.\n",
    "     - Increased likelihood of Type I errors (false positives).\n",
    "\n",
    "2. **Non-Normality of Residuals:**\n",
    "   - Omnibus and Jarque-Bera tests indicate residuals are not normally distributed, which affects inference validity.\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps to Address Issues**\n",
    "1. **Autocorrelation:**\n",
    "   - Use a Generalized Least Squares (GLS) model or Cochrane-Orcutt method to correct for autocorrelation.\n",
    "   - Alternatively, include lagged dependent variables to capture dynamics in the data.\n",
    "\n",
    "2. **Non-Normality:**\n",
    "   - Consider transforming the dependent variable (e.g., log transformation) or using robust regression techniques.\n",
    "\n",
    "3. **Reassess Model:**\n",
    "   - Investigate if additional explanatory variables or lagged terms can improve the model fit and address residual issues.\n",
    "\n",
    "By addressing these issues, the model's reliability and inference validity can be significantly improved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normality Assumption in OLS\n",
    "\n",
    "## **What Is the Normality Assumption?**\n",
    "- The assumption of **normality** states that the error term ($ \\epsilon $) in a regression model is normally distributed.\n",
    "- Normality is **not required** for the OLS estimator to be BLUE (Best Linear Unbiased Estimator).\n",
    "- However, it is important for:\n",
    "  - **Hypothesis Testing**: The $ t $-test assumes a normal distribution for the numerator.\n",
    "  - Simplifying algebra: A normal distribution is fully characterized by its mean and variance, making calculations straightforward.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Check for Normality?**\n",
    "- Departure from normality impacts:\n",
    "  - The **distribution of test statistics** (e.g., $ t $-tests may not follow the $ t $-distribution).\n",
    "  - The **inference procedure** (e.g., p-values become unreliable).\n",
    "- Errors are sensitive to **outliers**, which often cause violations of normality.\n",
    "\n",
    "---\n",
    "\n",
    "## **Moments of a Distribution**\n",
    "1. **Skewness:**\n",
    "   - **Definition:** Measures the asymmetry of a distribution about its mean.\n",
    "   - **Types:**\n",
    "     - **Positive Skewness:** Tail is longer on the right side.\n",
    "     - **Negative Skewness:** Tail is longer on the left side.\n",
    "   - **Formula:**\n",
    "     $$\n",
    "     b_1 = \\frac{E(\\epsilon^3)}{\\sigma^3}\n",
    "     $$\n",
    "     - $ b_1 = 0 $ for symmetric distributions.\n",
    "\n",
    "2. **Kurtosis:**\n",
    "   - **Definition:** Measures the \"tailedness\" of a distribution.\n",
    "   - **Types:**\n",
    "     - **Mesokurtic:** Normal distribution; kurtosis = 3.\n",
    "     - **Leptokurtic:** More peaked, fatter tails; kurtosis > 3.\n",
    "     - **Platykurtic:** Flatter peak, thinner tails; kurtosis < 3.\n",
    "   - **Formula:**\n",
    "     $$\n",
    "     b_2 = \\frac{E(\\epsilon^4)}{\\sigma^4}\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "## **Bera-Jarque Test for Normality**\n",
    "- The Bera-Jarque (BJ) test formally checks for departures from normality by testing:\n",
    "  - Whether the **skewness** ($ b_1 $) and **excess kurtosis** ($ b_2 - 3 $) are jointly equal to zero.\n",
    "- **Test Statistic:**\n",
    "  $$\n",
    "  W = T \\left( \\frac{b_1^2}{6} + \\frac{(b_2 - 3)^2}{24} \\right) \\sim \\chi^2_2\n",
    "  $$\n",
    "  - $ T $: Sample size.\n",
    "  - $ W $: Follows a Chi-squared distribution with 2 degrees of freedom under the null hypothesis.\n",
    "- **Null Hypothesis ($ H_0 $):**\n",
    "  - The residuals are normally distributed ($ b_1 = 0 $ and $ b_2 = 3 $).\n",
    "- **Steps:**\n",
    "  1. Estimate $ b_1 $ and $ b_2 $ using residuals from the OLS regression.\n",
    "  2. Compute $ W $ and compare with the critical value from the Chi-squared distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## **What to Do If Normality Is Violated?**\n",
    "1. **Switch to Methods That Don't Assume Normality:**\n",
    "   - Use **robust statistical methods** like bootstrapping.\n",
    "   - Employ **non-parametric tests** that are not sensitive to distributional assumptions.\n",
    "\n",
    "2. **Handle Outliers:**\n",
    "   - Outliers often cause departures from normality.\n",
    "   - Use **dummy variables** to account for specific observations causing extreme residuals.\n",
    "\n",
    "3. **Transform Variables:**\n",
    "   - Apply transformations (e.g., logarithms or square roots) to reduce skewness or normalize the residuals.\n",
    "\n",
    "4. **Check Robustness:**\n",
    "   - If normality is violated but only marginally, it often does not invalidate results, especially with large sample sizes (due to the Central Limit Theorem).\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "- **Key Points:**\n",
    "  - Normality is not critical for OLS to be BLUE but is important for hypothesis testing and p-values.\n",
    "  - Normality violations often arise due to outliers or non-symmetric distributions in errors.\n",
    "- **BJ Test Formula:**\n",
    "  $$\n",
    "  W = T \\left( \\frac{b_1^2}{6} + \\frac{(b_2 - 3)^2}{24} \\right) \\sim \\chi^2_2\n",
    "  $$\n",
    "- **Practical Implications:**\n",
    "  - Check residuals for normality using the BJ test.\n",
    "  - Address outliers or use robust estimation methods if normality is violated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use Ordinary Least Squares (OLS)\n",
    "\n",
    "Ordinary Least Squares (OLS) is a foundational statistical method for estimating the relationships between variables. However, its applicability depends on the nature of the data and the research question. Below are the scenarios where OLS is appropriate:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Continuous Dependent Variable**\n",
    "   - OLS is most suitable when the dependent variable is continuous and unbounded (e.g., income, weight, temperature).\n",
    "   - Example: Estimating the relationship between hours studied and exam scores.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Linear Relationship**\n",
    "   - The relationship between the dependent variable (`y`) and the independent variables (`x`) should be linear.\n",
    "   - Example: The impact of advertising expenditure on sales, where sales increase linearly with more advertising.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **No Multicollinearity**\n",
    "   - OLS assumes that the independent variables are not highly correlated with each other.\n",
    "   - High multicollinearity can distort coefficient estimates and their statistical significance.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Homoscedasticity**\n",
    "   - The variance of the error term should be constant across all levels of the independent variable(s).\n",
    "   - If this assumption is violated (heteroscedasticity), robust standard errors or alternative estimators should be used.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **No Autocorrelation**\n",
    "   - The error terms should not be correlated across observations.\n",
    "   - This is particularly important in time-series data where autocorrelation can bias OLS results.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Large Sample Size**\n",
    "   - OLS performs well with large datasets because it minimizes the variance of estimators under the Gauss-Markov theorem.\n",
    "   - Small samples may lead to unreliable results, particularly if other assumptions are borderline valid.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Dependent Variable's Range**\n",
    "   - The dependent variable should not have bounded or binary values.\n",
    "   - For binary dependent variables (e.g., yes/no decisions), models like logit or probit are preferred.\n",
    "\n",
    "---\n",
    "\n",
    "## When Not to Use OLS\n",
    "- **Binary or Categorical Dependent Variables**:\n",
    "  - For binary outcomes (e.g., voting: yes/no), use logit or probit models instead of OLS.\n",
    "- **Dynamic Panel Data**:\n",
    "  - When lagged dependent variables are included, OLS can be biased and inconsistent. Use Generalized Method of Moments (GMM).\n",
    "- **Violation of Assumptions**:\n",
    "  - When any of the key OLS assumptions (e.g., linearity, no heteroscedasticity) are significantly violated.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "Use OLS when:\n",
    "1. The dependent variable is continuous and unbounded.\n",
    "2. The relationship between variables is linear.\n",
    "3. The assumptions of homoscedasticity, no multicollinearity, and no autocorrelation are met.\n",
    "\n",
    "For non-linear relationships, bounded outcomes, or violations of assumptions, alternative estimation techniques should be considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:38: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  t_stat_intercept_disney = capm_disney.tvalues[0]\n",
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:39: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_intercept_disney = capm_disney.pvalues[0]\n",
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:41: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  t_stat_intercept_ge = capm_ge.tvalues[0]\n",
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:42: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_intercept_ge = capm_ge.pvalues[0]\n",
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:45: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  t_stat_beta_disney = capm_disney.tvalues[1]\n",
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_beta_disney = capm_disney.pvalues[1]\n",
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:48: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  t_stat_beta_ge = capm_ge.tvalues[1]\n",
      "C:\\Users\\moeth\\AppData\\Local\\Temp\\ipykernel_2308\\2845432002.py:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  p_value_beta_ge = capm_ge.pvalues[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Disney CAPM Model': <class 'statsmodels.iolib.summary.Summary'>\n",
       " \"\"\"\n",
       "                             OLS Regression Results                            \n",
       " ==============================================================================\n",
       " Dep. Variable:          Excess_Return   R-squared:                       0.445\n",
       " Model:                            OLS   Adj. R-squared:                  0.444\n",
       " Method:                 Least Squares   F-statistic:                     343.3\n",
       " Date:                Sat, 14 Dec 2024   Prob (F-statistic):           1.04e-56\n",
       " Time:                        15:44:33   Log-Likelihood:                -1358.9\n",
       " No. Observations:                 430   AIC:                             2722.\n",
       " Df Residuals:                     428   BIC:                             2730.\n",
       " Df Model:                           1                                         \n",
       " Covariance Type:            nonrobust                                         \n",
       " =================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       " ---------------------------------------------------------------------------------\n",
       " const             0.6454      0.277      2.328      0.020       0.100       1.190\n",
       " Excess_Market     1.1368      0.061     18.529      0.000       1.016       1.257\n",
       " ==============================================================================\n",
       " Omnibus:                       50.802   Durbin-Watson:                   2.137\n",
       " Prob(Omnibus):                  0.000   Jarque-Bera (JB):              120.540\n",
       " Skew:                           0.611   Prob(JB):                     6.69e-27\n",
       " Kurtosis:                       5.288   Cond. No.                         4.55\n",
       " ==============================================================================\n",
       " \n",
       " Notes:\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       " \"\"\",\n",
       " 'General Electric CAPM Model': <class 'statsmodels.iolib.summary.Summary'>\n",
       " \"\"\"\n",
       "                             OLS Regression Results                            \n",
       " ==============================================================================\n",
       " Dep. Variable:          Excess_Return   R-squared:                       0.432\n",
       " Model:                            OLS   Adj. R-squared:                  0.431\n",
       " Method:                 Least Squares   F-statistic:                     325.4\n",
       " Date:                Sat, 14 Dec 2024   Prob (F-statistic):           1.61e-54\n",
       " Time:                        15:44:33   Log-Likelihood:                -1375.6\n",
       " No. Observations:                 430   AIC:                             2755.\n",
       " Df Residuals:                     428   BIC:                             2763.\n",
       " Df Model:                           1                                         \n",
       " Covariance Type:            nonrobust                                         \n",
       " =================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       " ---------------------------------------------------------------------------------\n",
       " const             0.0594      0.288      0.206      0.837      -0.507       0.626\n",
       " Excess_Market     1.1506      0.064     18.040      0.000       1.025       1.276\n",
       " ==============================================================================\n",
       " Omnibus:                       50.887   Durbin-Watson:                   1.773\n",
       " Prob(Omnibus):                  0.000   Jarque-Bera (JB):              294.118\n",
       " Skew:                          -0.258   Prob(JB):                     1.36e-64\n",
       " Kurtosis:                       7.019   Cond. No.                         4.55\n",
       " ==============================================================================\n",
       " \n",
       " Notes:\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       " \"\"\",\n",
       " 'Hypothesis Testing': {'Disney Intercept': (2.3277964628526853,\n",
       "   0.020388244824234392),\n",
       "  'Disney Beta': (18.52944951660638, 1.044164010940205e-56),\n",
       "  'Disney Joint Test (F-stat)': (343.3404993884638, 1.0441640109404312e-56),\n",
       "  'GE Intercept': (0.20611680500060686, 0.8367976803775333),\n",
       "  'GE Beta': (18.03986854752062, 1.6138918322048617e-54),\n",
       "  'GE Joint Test (F-stat)': (325.4368572118237, 1.613891832204861e-54)},\n",
       " 'Diagnostic Tests': {'Disney Normality (JB)': (120.53959957193229,\n",
       "   6.685879231038203e-27),\n",
       "  'GE Normality (JB)': (294.1183280698712, 1.3583651983850218e-64),\n",
       "  'Disney Heteroskedasticity (BP)': (0.15388288286238727,\n",
       "   0.6948520133039783,\n",
       "   0.15322198164033812,\n",
       "   0.695669940391147),\n",
       "  'GE Heteroskedasticity (BP)': (5.325047777894724,\n",
       "   0.021021032432546088,\n",
       "   5.366740932125825,\n",
       "   0.020995666211821903),\n",
       "  'Disney Autocorrelation (DW)': 2.1367243650871215,\n",
       "  'GE Autocorrelation (DW)': 1.773274684807596,\n",
       "  'Disney Multicollinearity (VIF)': [1.011672105953917, 0.9999999999999998],\n",
       "  'GE Multicollinearity (VIF)': [1.011672105953917, 0.9999999999999998]}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.stattools import jarque_bera, durbin_watson\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Load data\n",
    "disney_data_path = \"data/Disney Data.xlsx\"\n",
    "ge_data_path = \"data/General Electric Data.xlsx\"\n",
    "disney_data = pd.read_excel(disney_data_path)\n",
    "ge_data = pd.read_excel(ge_data_path)\n",
    "\n",
    "# Convert 'date' column to datetime for clarity\n",
    "disney_data['Date'] = pd.to_datetime(disney_data['date'])\n",
    "ge_data['Date'] = pd.to_datetime(ge_data['date'])\n",
    "\n",
    "# Calculate returns for both datasets\n",
    "disney_data['Return'] = disney_data['disney'].pct_change() * 100\n",
    "ge_data['Return'] = ge_data['ge'].pct_change() * 100\n",
    "\n",
    "# CAPM Model for Disney\n",
    "disney_data['Excess_Return'] = disney_data['Return'] - disney_data['rf']\n",
    "disney_data['Excess_Market'] = disney_data['mktrf'] - disney_data['rf']\n",
    "X_disney_capm = sm.add_constant(disney_data['Excess_Market'])\n",
    "y_disney_capm = disney_data['Excess_Return']\n",
    "capm_disney = sm.OLS(y_disney_capm, X_disney_capm, missing='drop').fit()\n",
    "\n",
    "# CAPM Model for General Electric\n",
    "ge_data['Excess_Return'] = ge_data['Return'] - ge_data['rf']\n",
    "ge_data['Excess_Market'] = ge_data['mktrf'] - ge_data['rf']\n",
    "X_ge_capm = sm.add_constant(ge_data['Excess_Market'])\n",
    "y_ge_capm = ge_data['Excess_Return']\n",
    "capm_ge = sm.OLS(y_ge_capm, X_ge_capm, missing='drop').fit()\n",
    "\n",
    "# Hypothesis Testing\n",
    "# 1. Test for Intercept\n",
    "t_stat_intercept_disney = capm_disney.tvalues[0]\n",
    "p_value_intercept_disney = capm_disney.pvalues[0]\n",
    "\n",
    "t_stat_intercept_ge = capm_ge.tvalues[0]\n",
    "p_value_intercept_ge = capm_ge.pvalues[0]\n",
    "\n",
    "# 2. Test for Beta Coefficient\n",
    "t_stat_beta_disney = capm_disney.tvalues[1]\n",
    "p_value_beta_disney = capm_disney.pvalues[1]\n",
    "\n",
    "t_stat_beta_ge = capm_ge.tvalues[1]\n",
    "p_value_beta_ge = capm_ge.pvalues[1]\n",
    "\n",
    "# 3. Joint Test for Intercept and Beta Coefficient\n",
    "f_stat_disney = capm_disney.fvalue\n",
    "f_pvalue_disney = capm_disney.f_pvalue\n",
    "\n",
    "f_stat_ge = capm_ge.fvalue\n",
    "f_pvalue_ge = capm_ge.f_pvalue\n",
    "\n",
    "# Diagnostic Tests\n",
    "# 1. Normality Test (Jarque-Bera)\n",
    "jb_stat_disney, jb_pvalue_disney, _, _ = jarque_bera(capm_disney.resid)\n",
    "jb_stat_ge, jb_pvalue_ge, _, _ = jarque_bera(capm_ge.resid)\n",
    "\n",
    "# 2. Heteroskedasticity Test (Breusch-Pagan)\n",
    "bp_test_disney = het_breuschpagan(capm_disney.resid, capm_disney.model.exog)\n",
    "bp_test_ge = het_breuschpagan(capm_ge.resid, capm_ge.model.exog)\n",
    "\n",
    "# 3. Autocorrelation Test (Durbin-Watson)\n",
    "dw_stat_disney = durbin_watson(capm_disney.resid)\n",
    "dw_stat_ge = durbin_watson(capm_ge.resid)\n",
    "\n",
    "# 4. Multicollinearity Test (Variance Inflation Factor)\n",
    "vif_disney = [variance_inflation_factor(X_disney_capm.values, i) for i in range(X_disney_capm.shape[1])]\n",
    "vif_ge = [variance_inflation_factor(X_ge_capm.values, i) for i in range(X_ge_capm.shape[1])]\n",
    "\n",
    "# Output Results\n",
    "results = {\n",
    "    \"Disney CAPM Model\": capm_disney.summary(),\n",
    "    \"General Electric CAPM Model\": capm_ge.summary(),\n",
    "    \"Hypothesis Testing\": {\n",
    "        \"Disney Intercept\": (t_stat_intercept_disney, p_value_intercept_disney),\n",
    "        \"Disney Beta\": (t_stat_beta_disney, p_value_beta_disney),\n",
    "        \"Disney Joint Test (F-stat)\": (f_stat_disney, f_pvalue_disney),\n",
    "        \"GE Intercept\": (t_stat_intercept_ge, p_value_intercept_ge),\n",
    "        \"GE Beta\": (t_stat_beta_ge, p_value_beta_ge),\n",
    "        \"GE Joint Test (F-stat)\": (f_stat_ge, f_pvalue_ge),\n",
    "    },\n",
    "    \"Diagnostic Tests\": {\n",
    "        \"Disney Normality (JB)\": (jb_stat_disney, jb_pvalue_disney),\n",
    "        \"GE Normality (JB)\": (jb_stat_ge, jb_pvalue_ge),\n",
    "        \"Disney Heteroskedasticity (BP)\": bp_test_disney,\n",
    "        \"GE Heteroskedasticity (BP)\": bp_test_ge,\n",
    "        \"Disney Autocorrelation (DW)\": dw_stat_disney,\n",
    "        \"GE Autocorrelation (DW)\": dw_stat_ge,\n",
    "        \"Disney Multicollinearity (VIF)\": vif_disney,\n",
    "        \"GE Multicollinearity (VIF)\": vif_ge,\n",
    "    },\n",
    "}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Disney and General Electric CAPM Models\n",
    "\n",
    "## **Disney CAPM Model**\n",
    "\n",
    "### **Key Results:**\n",
    "- **R-squared:** 0.445\n",
    "  - Approximately 44.5% of the variation in Disney's excess returns is explained by the excess market returns.\n",
    "- **Intercept ($ \\alpha $):** 0.6454 (significant at $ p = 0.020 $)\n",
    "  - Indicates a positive excess return that cannot be explained by market movements.\n",
    "- **Beta ($ \\beta $):** 1.1368 (highly significant at $ p < 0.001 $)\n",
    "  - Suggests that Disney's returns are slightly more volatile than the market.\n",
    "\n",
    "### **Model Diagnostics:**\n",
    "- **Normality of Residuals (Jarque-Bera Test):**\n",
    "  - $ JB = 120.54 $, $ p < 0.001 $\n",
    "  - Residuals are not normally distributed, indicating potential non-linearity or outliers.\n",
    "- **Heteroskedasticity (Breusch-Pagan Test):**\n",
    "  - $ p = 0.695 $\n",
    "  - No evidence of heteroskedasticity.\n",
    "- **Autocorrelation (Durbin-Watson Statistic):**\n",
    "  - $ DW = 2.137 $\n",
    "  - Residuals do not exhibit significant autocorrelation.\n",
    "- **Multicollinearity (Variance Inflation Factor):**\n",
    "  - VIF values close to 1 indicate no multicollinearity issues.\n",
    "\n",
    "---\n",
    "\n",
    "## **General Electric CAPM Model**\n",
    "\n",
    "### **Key Results:**\n",
    "- **R-squared:** 0.432\n",
    "  - Approximately 43.2% of the variation in GE's excess returns is explained by the excess market returns.\n",
    "- **Intercept ($ \\alpha $):** 0.0594 (not significant at $ p = 0.837 $)\n",
    "  - Suggests no additional return component beyond market movements.\n",
    "- **Beta ($ \\beta $):** 1.1506 (highly significant at $ p < 0.001 $)\n",
    "  - Indicates that GE's returns are slightly more volatile than the market.\n",
    "\n",
    "### **Model Diagnostics:**\n",
    "- **Normality of Residuals (Jarque-Bera Test):**\n",
    "  - $ JB = 294.12 $, $ p < 0.001 $\n",
    "  - Residuals are not normally distributed, similar to Disney.\n",
    "- **Heteroskedasticity (Breusch-Pagan Test):**\n",
    "  - $ p = 0.021 $\n",
    "  - Evidence of heteroskedasticity in the residuals.\n",
    "- **Autocorrelation (Durbin-Watson Statistic):**\n",
    "  - $ DW = 1.773 $\n",
    "  - Indicates positive autocorrelation in residuals.\n",
    "- **Multicollinearity (Variance Inflation Factor):**\n",
    "  - VIF values close to 1 suggest no multicollinearity issues.\n",
    "\n",
    "---\n",
    "\n",
    "## **Hypothesis Testing**\n",
    "\n",
    "### **Disney CAPM Model:**\n",
    "- **Intercept Test:** $ t = 2.33 $, $ p = 0.020 $\n",
    "  - The intercept ($ \\alpha $) is statistically significant, suggesting additional returns beyond the market.\n",
    "- **Beta Coefficient Test:** $ t = 18.53 $, $ p < 0.001 $\n",
    "  - The beta ($ \\beta $) is highly significant, indicating strong market sensitivity.\n",
    "- **Joint Test (F-statistic):** $ F = 343.34 $, $ p < 0.001 $\n",
    "  - Both the intercept and beta are jointly significant.\n",
    "\n",
    "### **GE CAPM Model:**\n",
    "- **Intercept Test:** $ t = 0.21 $, $ p = 0.837 $\n",
    "  - The intercept ($ \\alpha $) is not statistically significant, indicating no unexplained return.\n",
    "- **Beta Coefficient Test:** $ t = 18.04 $, $ p < 0.001 $\n",
    "  - The beta ($ \\beta $) is highly significant, indicating strong market sensitivity.\n",
    "- **Joint Test (F-statistic):** $ F = 325.44 $, $ p < 0.001 $\n",
    "  - Beta is the primary driver of excess returns.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "1. **Disney Model:**\n",
    "   - Disney has a significant intercept, indicating the potential for excess returns beyond market performance.\n",
    "   - The beta suggests Disney's returns are slightly more volatile than the market.\n",
    "   - The model does not suffer from heteroskedasticity or autocorrelation, but residuals are not normally distributed.\n",
    "\n",
    "2. **GE Model:**\n",
    "   - GE's intercept is not significant, meaning returns are closely aligned with market performance.\n",
    "   - The beta shows similar volatility to Disney.\n",
    "   - The model exhibits heteroskedasticity and positive autocorrelation, which require further adjustment.\n",
    "\n",
    "3. **Key Diagnostic Insights:**\n",
    "   - Both models have issues with non-normal residuals, which could be addressed by transforming the dependent variable or using robust standard errors.\n",
    "   - The GE modelâ€™s heteroskedasticity and autocorrelation issues may require a generalized least squares (GLS) approach or ARIMA modeling for the residuals.\n",
    "\n",
    "Overall, while both models effectively capture market sensitivity, the diagnostic results suggest the need for refinements, particularly in the GE model, to ensure valid inference and robust results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
